{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02-topic-modelling.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOdkwo8rThDHmF964lm2f7M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/practical-natural-language-processing/blob/master/7-topics-in-brief/02_topic_modelling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3olgxrQkYkbf"
      },
      "source": [
        "##Topic Modelling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jLZWECaYlh5"
      },
      "source": [
        "Topic modeling is one of the most common applications of NLP in industrial use\n",
        "cases. For analyzing different forms of text from news articles to tweets, from visualizing word clouds to creating graphs of connected topics and documents,\n",
        "topic models are useful for a range of use cases. Topic models are used extensively for document clustering and organizing large collections of text data. They’re also useful for text classification.\n",
        "\n",
        "One way to approach it is to bring out some words that best describe the corpus,\n",
        "like the most common words in the corpus. This is called a word cloud. The key\n",
        "to a good word cloud is to remove stop words. If we take any English text corpus and list out the most frequent k words, we won’t get any meaningful insights, as the most frequent words will be stop words (the, is, are, am, etc.). After doing appropriate preprocessing, the word cloud may yield some meaningful insights depending on the document collection.\n",
        "\n",
        "Topic modeling operationalizes this intuition. It tries to identify the “key” words (called “topics”) present in a text corpus without prior knowledge about it, unlike the rule-based text mining approaches that use regular expressions or dictionary-based keyword searching techniques.\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/practical-nlp/7-3.png?raw=1' width='800'/>\n",
        "\n",
        "Topic modeling generally refers to a collection of unsupervised statistical learning methods to discover latent topics in a large collection of text documents. Some of the popular topic modeling algorithms are latent Dirichlet allocation (LDA), latent semantic analysis (LSA), and probabilistic latent semantic analysis (PLSA). In practice, the technique that’s most commonly used is LDA.\n",
        "\n",
        "Let’s start with a toy corpus. Say we have a collection of documents, D1 to D5, and each document consists of a single sentence:\n",
        "\n",
        "- D1: I like to eat broccoli and bananas.\n",
        "- D2: I ate a banana and salad for breakfast.\n",
        "- D3: Puppies and kittens are cute.\n",
        "- D4: My sister adopted a kitten yesterday.\n",
        "- D5: Look at this cute hamster munching on a piece of broccoli.\n",
        "\n",
        "Learning a topic model on this collection using LDA may produce an output like this:\n",
        "\n",
        "- Topic A: 30% broccoli, 15% bananas, 10% breakfast, 10% munching\n",
        "- Topic B: 20% puppies, 20% kittens, 20% cute, 15% hamster\n",
        "\n",
        "- Document 1 and 2: 100% Topic A\n",
        "- Document 3 and 4: 100% Topic B\n",
        "- Document 5: 60% Topic A, 40% Topic B\n",
        "\n",
        "Thus, topics are nothing but a mixture of keywords with a probability distribution, and documents are made up of a mixture of topics, again with a probability distribution.\n",
        "\n",
        "A topic model only gives a collection of keywords per topic. What exactly the\n",
        "topic represents and what it should be named is typically left to human interpretation in an LDA model. Here, we might look at Topic A and say, “it is about food.” Likewise, for topic B, we might say, “it is about pets.”\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kg9Tx2Fldlqa"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YAqWTEPxdm5S",
        "outputId": "febc8e35-bde7-41b0-cba7-01a3f1576a72"
      },
      "source": [
        "import os\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from gensim.models import LdaModel\n",
        "from gensim.corpora import Dictionary\n",
        "from pprint import pprint"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owE7G8hHe4VU"
      },
      "source": [
        "## Training topic model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FeklF12fDd1"
      },
      "source": [
        "Here, we’ll use an LDA implementation from the Python library gensim and the CMU Book Summary Dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yaQ4jpoDd3sF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}