{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5-using-pre-trained-word2vec-model.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPlR4c0bH8r3dlsMRuk7UnL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/practical-natural-language-processing/blob/chapter-3-text-representation/5_using_pre_trained_word2vec_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ds66UJ-gX_Mb",
        "colab_type": "text"
      },
      "source": [
        "# Using a pre-trained word2vec model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqYICccZYbNE",
        "colab_type": "text"
      },
      "source": [
        "## Word Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kehug1ONYcIG",
        "colab_type": "text"
      },
      "source": [
        "What does it mean when we say a text representation should capture “distributional similarities between words”? \n",
        "\n",
        "Let’s consider some examples. If we’re given the word “USA,” distributionally similar words could be other countries (e.g., Canada, Germany, India, etc.) or cities in the USA. If we’re given the word “beautiful,” words that share some relationship with this word (e.g., synonyms, antonyms) could be considered distributionally similar words. These are words that are likely to occur in similar contexts. \n",
        "\n",
        "In 2013, a seminal work by Mikolov et al. showed that their neural network–based word representation model known as “Word2vec,” based on “distributional similarity,” can capture word analogy relationships such as:\n",
        "\n",
        "`King – Man + Woman ≈ Queen`\n",
        "\n",
        "While learning such semantically rich relationships, Word2vec ensures that the learned word representations are low dimensional (vectors of dimensions 50–500, instead of several thousands) and dense (that is, most values in these vectors are non-zero). \n",
        "\n",
        "Such representations make ML tasks more tractable and efficient. Word2vec led to a lot of work (both pure and applied) in the direction of learning text representations using neural networks. These representations are also called “embeddings.” \n",
        "\n",
        "To “derive” the meaning of the word, Word2vec uses distributional similarity and distributional hypothesis. That is, it derives the meaning of a word from its context: words that appear in its neighborhood in the text. So, if two different words (often) occur in similar context, then it’s highly likely that their meanings are also similar. \n",
        "\n",
        "Word2vec operationalizes this by projecting the meaning of the words in a vector space where words with similar meanings will tend to cluster together, and words with very different meanings are far from one another.\n",
        "\n",
        "Conceptually, Word2vec takes a large corpus of text as input and “learns” to represent the words in a common vector space based on the contexts in which they appear in the corpus. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOKzKwpgaDCm",
        "colab_type": "text"
      },
      "source": [
        "## Pre-trained word embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWZ7ymMXaD6Q",
        "colab_type": "text"
      },
      "source": [
        "Some of the most popular pre-trained embeddings are Word2vec by Google, GloVe by Stanford, and fasttext embeddings by Facebook, to name a few. Further, they’re available for various dimensions like d = 25, 50, 100, 200, 300, 600.\n",
        "\n",
        "Let us take an example of a pre-trained word2vec model, and how we can use it to look for most similar words. We will use the Google News vectors embeddings. https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM\n",
        "\n",
        "A few other pre-trained word embedding models, and details on the means to access them through gensim can be found in: https://github.com/RaRe-Technologies/gensim-data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSImRSwWadAU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "631b551b-a789-447d-eeb6-417a76d34782"
      },
      "source": [
        "!wget -P /tmp/input/ -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-08-28 09:33:59--  https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.226.91\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.226.91|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1647046227 (1.5G) [application/x-gzip]\n",
            "Saving to: ‘/tmp/input/GoogleNews-vectors-negative300.bin.gz’\n",
            "\n",
            "GoogleNews-vectors- 100%[===================>]   1.53G  34.4MB/s    in 46s     \n",
            "\n",
            "2020-08-28 09:34:45 (34.1 MB/s) - ‘/tmp/input/GoogleNews-vectors-negative300.bin.gz’ saved [1647046227/1647046227]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iI5CJSgSal59",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#This module ignores the various types of warnings generated\n",
        "import warnings \n",
        "warnings.filterwarnings(\"ignore\") \n",
        "\n",
        "#This module provides a way of using operating system dependent functionality\n",
        "import os \n",
        "\n",
        "#This module helps in retrieving information on running processes and system resource utilization\n",
        "import psutil \n",
        "process = psutil.Process(os.getpid())\n",
        "from psutil import virtual_memory\n",
        "mem = virtual_memory()\n",
        "\n",
        "#This module is used to calculate the time\n",
        "import time "
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6LhoxT0a9iS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "47823d1e-5c10-4ccf-d25f-da0fed374b4c"
      },
      "source": [
        "from gensim.models import Word2Vec, KeyedVectors\n",
        "\n",
        "pretrainedpath = '/tmp/input/GoogleNews-vectors-negative300.bin.gz'\n",
        "\n",
        "#Load W2V model. This will take some time, but it is a one time effort! \n",
        "pre = process.memory_info().rss\n",
        "print(\"Memory used in GB before Loading the Model: %0.2f\"%float(pre/(10**9))) #Check memory usage before loading the model\n",
        "print('-'*10)\n",
        "\n",
        "start_time = time.time() #Start the timer\n",
        "ttl = mem.total #Toal memory available\n",
        "\n",
        "#load the model\n",
        "w2v_model = KeyedVectors.load_word2vec_format(pretrainedpath, binary=True) \n",
        "print(\"%0.2f seconds taken to load\"%float(time.time() - start_time)) #Calculate the total time elapsed since starting the timer\n",
        "print('-'*10)\n",
        "\n",
        "print('Finished loading Word2Vec')\n",
        "print('-'*10)\n",
        "\n",
        "post = process.memory_info().rss\n",
        "print(\"Memory used in GB after Loading the Model: {:.2f}\".format(float(post/(10**9)))) #Calculate the memory used after loading the model\n",
        "print('-'*10)\n",
        "\n",
        "print(\"Percentage increase in memory usage: {:.2f}% \".format(float((post/pre)*100))) #Percentage increase in memory after loading the model\n",
        "print('-'*10)\n",
        "\n",
        "print(\"Numver of words in vocablulary: \",len(w2v_model.vocab)) #Number of words in the vocabulary."
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Memory used in GB before Loading the Model: 0.16\n",
            "----------\n",
            "106.34 seconds taken to load\n",
            "----------\n",
            "Finished loading Word2Vec\n",
            "----------\n",
            "Memory used in GB after Loading the Model: 5.01\n",
            "----------\n",
            "Percentage increase in memory usage: 3100.78% \n",
            "----------\n",
            "Numver of words in vocablulary:  3000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-mmhDeLbglI",
        "colab_type": "text"
      },
      "source": [
        "Here, we find the words that are semantically most similar to the word “beautiful”; the last line returns the embedding vector of the word “beautiful”.\n",
        "\n",
        "Let us examine the model by knowing what the most similar words are, for a given word!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIEzJufYbYuV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "1dc50926-96c2-4049-ce32-6d1004048b45"
      },
      "source": [
        "w2v_model.most_similar('beautiful')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('gorgeous', 0.8353004455566406),\n",
              " ('lovely', 0.810693621635437),\n",
              " ('stunningly_beautiful', 0.7329413890838623),\n",
              " ('breathtakingly_beautiful', 0.7231341004371643),\n",
              " ('wonderful', 0.6854087114334106),\n",
              " ('fabulous', 0.6700063943862915),\n",
              " ('loveliest', 0.6612576246261597),\n",
              " ('prettiest', 0.6595001816749573),\n",
              " ('beatiful', 0.6593326330184937),\n",
              " ('magnificent', 0.6591402292251587)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "actJE1Tlb8BV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "9b392eb1-718c-4a75-8fc6-e362511843c5"
      },
      "source": [
        "# Let us try with another word! \n",
        "w2v_model.most_similar('toronto')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('montreal', 0.698411226272583),\n",
              " ('vancouver', 0.6587257385253906),\n",
              " ('nyc', 0.6248831748962402),\n",
              " ('alberta', 0.6179691553115845),\n",
              " ('boston', 0.611499547958374),\n",
              " ('calgary', 0.61032634973526),\n",
              " ('edmonton', 0.6100261211395264),\n",
              " ('canadian', 0.5944076776504517),\n",
              " ('chicago', 0.5911980271339417),\n",
              " ('springfield', 0.5888351202011108)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKNwhcoRcGqC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d8cc0b88-77c2-4d1f-cd3c-9125137100d0"
      },
      "source": [
        "# What is the vector representation for a word? \n",
        "w2v_model['beautiful']"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.01831055,  0.05566406, -0.01153564,  0.07275391,  0.15136719,\n",
              "       -0.06176758,  0.20605469, -0.15332031, -0.05908203,  0.22851562,\n",
              "       -0.06445312, -0.22851562, -0.09472656, -0.03344727,  0.24707031,\n",
              "        0.05541992, -0.00921631,  0.1328125 , -0.15429688,  0.08105469,\n",
              "       -0.07373047,  0.24316406,  0.12353516, -0.09277344,  0.08203125,\n",
              "        0.06494141,  0.15722656,  0.11279297, -0.0612793 , -0.296875  ,\n",
              "       -0.13378906,  0.234375  ,  0.09765625,  0.17773438,  0.06689453,\n",
              "       -0.27539062,  0.06445312, -0.13867188, -0.08886719,  0.171875  ,\n",
              "        0.07861328, -0.10058594,  0.23925781,  0.03808594,  0.18652344,\n",
              "       -0.11279297,  0.22558594,  0.10986328, -0.11865234,  0.02026367,\n",
              "        0.11376953,  0.09570312,  0.29492188,  0.08251953, -0.05444336,\n",
              "       -0.0090332 , -0.0625    , -0.17578125, -0.08154297,  0.01062012,\n",
              "       -0.04736328, -0.08544922, -0.19042969, -0.30273438,  0.07617188,\n",
              "        0.125     , -0.05932617,  0.03833008, -0.03564453,  0.2421875 ,\n",
              "        0.36132812,  0.04760742,  0.00631714, -0.03088379, -0.13964844,\n",
              "        0.22558594, -0.06298828, -0.02636719,  0.1171875 ,  0.33398438,\n",
              "       -0.07666016, -0.06689453,  0.04150391, -0.15136719, -0.22460938,\n",
              "        0.03320312, -0.15332031,  0.07128906,  0.16992188,  0.11572266,\n",
              "       -0.13085938,  0.12451172, -0.20410156,  0.04736328, -0.296875  ,\n",
              "       -0.17480469,  0.00872803, -0.04638672,  0.10791016, -0.203125  ,\n",
              "       -0.27539062,  0.2734375 ,  0.02563477, -0.11035156,  0.0625    ,\n",
              "        0.1953125 ,  0.16015625, -0.13769531, -0.09863281, -0.1953125 ,\n",
              "       -0.22851562,  0.25390625,  0.00915527, -0.03857422,  0.3984375 ,\n",
              "       -0.1796875 ,  0.03833008, -0.24804688,  0.03515625,  0.03881836,\n",
              "        0.03442383, -0.04101562,  0.20214844, -0.03015137, -0.09619141,\n",
              "        0.11669922, -0.06738281,  0.0625    ,  0.10742188,  0.25585938,\n",
              "       -0.21777344,  0.05639648, -0.0065918 ,  0.16113281,  0.11865234,\n",
              "       -0.03088379, -0.11572266,  0.02685547,  0.03100586,  0.09863281,\n",
              "        0.05883789,  0.00634766,  0.11914062,  0.07324219, -0.01586914,\n",
              "        0.18457031,  0.05322266,  0.19824219, -0.22363281, -0.25195312,\n",
              "        0.15039062,  0.22753906,  0.05737305,  0.16992188, -0.22558594,\n",
              "        0.06494141,  0.11914062, -0.06640625, -0.10449219, -0.07226562,\n",
              "       -0.16992188,  0.0625    ,  0.14648438,  0.27148438, -0.02172852,\n",
              "       -0.12695312,  0.18457031, -0.27539062, -0.36523438, -0.03491211,\n",
              "       -0.18554688,  0.23828125, -0.13867188,  0.00296021,  0.04272461,\n",
              "        0.13867188,  0.12207031,  0.05957031, -0.22167969, -0.18945312,\n",
              "       -0.23242188, -0.28710938, -0.00866699, -0.16113281, -0.24316406,\n",
              "        0.05712891, -0.06982422,  0.00053406, -0.10302734, -0.13378906,\n",
              "       -0.16113281,  0.11621094,  0.31640625, -0.02697754, -0.01574707,\n",
              "        0.11425781, -0.04174805,  0.05908203,  0.02661133, -0.08642578,\n",
              "        0.140625  ,  0.09228516, -0.25195312, -0.31445312, -0.05688477,\n",
              "        0.01031494,  0.0234375 , -0.02331543, -0.08056641,  0.01269531,\n",
              "       -0.34179688,  0.17285156, -0.16015625,  0.07763672, -0.03088379,\n",
              "        0.11962891,  0.11767578,  0.20117188, -0.01940918,  0.02172852,\n",
              "        0.23046875,  0.28125   , -0.17675781,  0.02978516,  0.08740234,\n",
              "       -0.06176758,  0.00939941, -0.09277344, -0.203125  ,  0.13085938,\n",
              "       -0.13671875, -0.00500488, -0.04296875,  0.12988281,  0.3515625 ,\n",
              "        0.0402832 , -0.12988281, -0.03173828,  0.28515625,  0.18261719,\n",
              "        0.13867188, -0.16503906, -0.26171875, -0.04345703,  0.0100708 ,\n",
              "        0.08740234,  0.00421143, -0.1328125 , -0.17578125, -0.04321289,\n",
              "       -0.015625  ,  0.16894531,  0.25      ,  0.37109375,  0.19921875,\n",
              "       -0.36132812, -0.10302734, -0.20800781, -0.20117188, -0.01519775,\n",
              "       -0.12207031, -0.12011719, -0.07421875, -0.04345703,  0.14160156,\n",
              "        0.15527344, -0.03027344, -0.09326172, -0.04589844,  0.16796875,\n",
              "       -0.03027344,  0.09179688, -0.10058594,  0.20703125,  0.11376953,\n",
              "       -0.12402344,  0.04003906,  0.06933594, -0.34570312,  0.03881836,\n",
              "        0.16210938,  0.05761719, -0.12792969, -0.05810547,  0.03857422,\n",
              "       -0.11328125, -0.1953125 , -0.28125   , -0.13183594,  0.15722656,\n",
              "       -0.09765625,  0.09619141, -0.09960938, -0.00285339, -0.03637695,\n",
              "        0.15429688,  0.06152344, -0.34570312,  0.11083984,  0.03344727],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IyQq9rEcmU3",
        "colab_type": "text"
      },
      "source": [
        "Note that if we search for a word that is not present in the Word2vec model, we’ll see a “key not found” error. \n",
        "\n",
        "Hence, as a good coding practice, it’s always advised to first check if the word is present in the model’s vocabulary before attempting to retrieve its vector. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jbx4tZoVcWe3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "outputId": "9fff57de-0bdc-4b20-89dd-a28a88847c51"
      },
      "source": [
        "# What if I am looking for a word that is not in this vocabulary?\n",
        "w2v_model.most_similar('practicalnlp')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-80d6cf5425de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# What if I am looking for a word that is not in this vocabulary?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mw2v_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'practicalnlp'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    529\u001b[0m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m                     \u001b[0mall_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    450\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"word 'practicalnlp' not in vocabulary\""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNckvoD4c0FJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "outputId": "3dbaa8cc-acbd-4013-c0ab-0e17b6e91a3f"
      },
      "source": [
        "w2v_model['practicalnlp']"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-02f42fa69a36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mw2v_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'practicalnlp'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, entities)\u001b[0m\n\u001b[1;32m    335\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0;31m# allow calls like trained_model['office'], as a shorthand for trained_model[['office']]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentity\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentity\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentities\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwords_closer_than\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    450\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"word 'practicalnlp' not in vocabulary\""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-P-lNMxPc6Ab",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "outputId": "17317f0a-1c47-4f2f-a6b5-f4525f661d76"
      },
      "source": [
        "w2v_model.most_similar('Ryaan')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-1b21fcb6c00c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mw2v_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Ryaan'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    529\u001b[0m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m                     \u001b[0mall_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    450\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"word 'Ryaan' not in vocabulary\""
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJBColx0dHbZ",
        "colab_type": "text"
      },
      "source": [
        "Two things to note while using pre-trained models:\n",
        "\n",
        "1. Tokens/Words are always lowercased. If a word is not in the vocabulary, the model throws an exception.\n",
        "2. So, it is always a good idea to encapsulate those statements in try/except blocks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ua3VgPEdhX1",
        "colab_type": "text"
      },
      "source": [
        "## Getting the embedding representation for full text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSoxkFXtdijy",
        "colab_type": "text"
      },
      "source": [
        "We have seen how to get embedding vectors for single words. How do we use them to get such a representation for a full text? A simple way is to just sum or average the embeddings for individual words. We will see an example of this using Word2Vec in Chapter 4. Let us see a small example using another NLP library Spacy - which we saw earlier in Chapter 2 too."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9l5fKXjKegud",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2eLoV9Wc-pN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "d4a82eb3-13c0-429a-8d36-2c698114d516"
      },
      "source": [
        "import spacy\n",
        "\n",
        "# Load the spacy model that we already installed in Chapter 2. This takes a few seconds.\n",
        "%time nlp = spacy.load('en_core_web_sm')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 693 ms, sys: 137 ms, total: 829 ms\n",
            "Wall time: 1.01 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJ1tXmE7drv0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# process a sentence using the model\n",
        "mydoc = nlp(\"Canada is a large country\")"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24x4JOeyfvcO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "outputId": "7f652cff-f946-427e-d9c1-0004a99cb051"
      },
      "source": [
        "# Get a vector for individual words\n",
        "print(mydoc[0].vector)  # vector for 'Canada', the first word in the text "
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0.21828675 -1.8616467  -1.8246782   3.9640498   3.1702113   3.4044275\n",
            "  0.01638174  1.0882976   4.298643    2.6220412   4.9655504  -0.55880976\n",
            "  0.40846556 -0.5311527  -1.936613    0.4890322   0.01409918  2.017309\n",
            "  1.9619753  -0.3345103  -2.0549996  -1.9780366   1.4000814  -3.9780545\n",
            " -1.4757934  -0.0758431  -0.15845299 -2.4204073  -0.22936638 -2.2050803\n",
            " -0.3578331  -1.9166979  -1.1512874  -2.2362876   3.0028348  -2.8118327\n",
            "  4.337387    0.9023212  -1.3791567   1.4033097   0.36686432  1.2876883\n",
            " -0.96107507 -4.0383596  -2.529714    1.3005439  -0.3787139  -0.9970173\n",
            "  0.580034    3.9643373  -0.5534672  -1.7696033  -1.928772   -1.1359324\n",
            " -4.5521493   2.0064287   3.4537764   0.8355992   2.3804865  -2.7131867\n",
            " -1.2354802  -0.41219887 -0.83612657  0.42878282  3.4744697  -0.57708937\n",
            "  2.292963   -4.568878    1.1227657   1.0749986  -3.4492185   1.0809329\n",
            " -1.3786955   0.21817788  1.15269    -0.93688565  1.9326314  -2.3508701\n",
            " -4.635231    0.07333571  1.2330925  -2.1642141  -1.450435   -1.3242478\n",
            "  3.841978    5.7909985  -2.2643628  -3.5615735  -0.12564743 -0.9254153\n",
            " -2.0761027  -2.4350288   0.51767826  5.4152255   4.686049    2.7099142 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nC66XBu9f8QW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "outputId": "3d2ed9c4-9452-4994-b322-bba09a457c5e"
      },
      "source": [
        "# Averaged vector for the entire sentence\n",
        "print(mydoc.vector)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-0.41918245  0.16200495 -0.79129374  1.4938685   1.3054081   2.0170395\n",
            "  1.5175831   0.93228465  2.1891901   1.4416349   0.06106711  0.09743678\n",
            " -0.2257375   0.17713971 -1.5512953   0.2645687  -1.0485003  -1.080864\n",
            " -0.8598553  -1.0081775   0.6843368  -0.6533736  -0.32018867 -0.45141286\n",
            " -1.5463241   0.8096622   0.66305393 -1.3783945   0.9841442  -0.6920743\n",
            "  0.22289622  0.15090446 -1.1180314  -1.9345913   0.38503057 -1.8199227\n",
            "  1.3990417  -1.0615559  -1.9546788  -0.2100529   1.9202824  -0.3749935\n",
            " -0.25492477 -2.1416779  -0.6990263  -0.03200452 -0.9764668   1.7387159\n",
            " -0.14891338  2.310504    2.7912867  -1.0199782   0.10864041  0.5835435\n",
            " -3.1004105   1.5820146   2.0926924  -0.40339088 -0.5991646  -1.3402617\n",
            " -0.7594353   0.2780761   2.629024    0.21917906  1.6852343   0.01221395\n",
            "  1.06303    -2.0580707   0.38551608 -1.1065528  -1.5958662  -0.6947671\n",
            " -1.4428566  -0.38099432  0.50667036  0.4246232   1.2565958  -0.09830198\n",
            " -1.0119321  -0.9477395   1.7218593  -1.3920348   0.73801583 -0.21935105\n",
            "  0.65630835  0.69767034 -0.8901253   0.2868591  -0.20612888 -0.09374499\n",
            " -0.8685352  -0.76120055  1.0453765   1.1798605   2.1939912   2.427199  ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FT2PckIvgBQb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "outputId": "047298bb-26b5-4d2f-bd63-738f1adc08b6"
      },
      "source": [
        "# What happens when I give a sentence with strange words (and stop words), and try to get its word vector in Spacy?\n",
        "temp = nlp(\"practicalnlp is a newword\")\n",
        "temp[0].vector"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 1.4743975 , -0.9622246 , -1.1067446 ,  0.71745956,  3.6869755 ,\n",
              "       -1.4803706 ,  2.6486013 , -0.02807039,  2.0256255 ,  3.9974196 ,\n",
              "        2.4013276 ,  2.8038695 ,  2.1188593 , -1.0725985 , -1.8718748 ,\n",
              "       -1.7074881 , -0.47109914,  1.753031  , -2.5303397 , -0.6910662 ,\n",
              "        1.4618394 ,  2.451487  , -2.729975  , -1.2108035 , -1.0596836 ,\n",
              "       -0.86415946, -1.8492069 , -1.3960482 ,  0.9203042 , -1.0206674 ,\n",
              "        2.9118538 , -1.1872265 ,  0.1711235 , -3.0739012 ,  1.3036946 ,\n",
              "       -2.8744037 ,  4.8433757 ,  0.5957062 , -2.63268   ,  1.5330828 ,\n",
              "        3.3766036 ,  2.9181588 , -1.454087  , -1.4249781 , -1.578454  ,\n",
              "        1.8532394 , -1.0139503 , -0.20046425, -0.5760678 ,  1.9376096 ,\n",
              "       -0.3732175 , -1.9566089 , -1.7265924 , -0.9403594 , -0.6440577 ,\n",
              "        1.1401592 ,  2.6202524 ,  0.08162385,  1.40631   ,  1.4846356 ,\n",
              "       -1.5503293 , -4.1603303 ,  0.78613114,  1.4033163 ,  2.3532844 ,\n",
              "       -0.48720837,  2.444285  , -3.873241  ,  2.10482   ,  2.8159425 ,\n",
              "       -0.5717292 ,  1.4221854 ,  0.8707961 ,  1.6473923 , -2.875568  ,\n",
              "       -1.1185746 ,  1.1086382 , -4.055404  , -2.004314  , -0.8189811 ,\n",
              "        0.10979348, -2.2864554 , -2.9955528 ,  1.1453302 , -1.5621811 ,\n",
              "        0.2975122 , -0.92439914, -2.4708936 ,  0.31517392, -1.6712105 ,\n",
              "        1.1495955 , -0.78865874, -3.1819708 ,  1.6073194 ,  2.9533925 ,\n",
              "        3.7986176 ], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBQlQjqigPH-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "outputId": "526a8773-39a4-44a5-c3f1-6322e53e6f25"
      },
      "source": [
        "temp = nlp(\"Ryaan is a king\")\n",
        "temp[0].vector"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 7.4579835e-02,  4.7708580e-01, -6.1412704e-01,  3.5218694e+00,\n",
              "        2.9170876e+00,  2.5409222e+00,  8.9063656e-01,  9.9246132e-01,\n",
              "        2.6454415e+00,  2.6988585e+00,  1.2484885e+00, -1.4995390e+00,\n",
              "        1.6782302e+00,  1.3731290e+00, -3.4341879e+00,  2.5433052e+00,\n",
              "        9.5429623e-01,  3.4910271e+00,  2.8027713e+00, -9.9670774e-01,\n",
              "        2.5236878e-01, -9.4356346e-01, -7.7272609e-02, -2.8652625e+00,\n",
              "       -8.0851555e-01, -1.2593380e+00, -2.4018660e+00, -1.9876711e+00,\n",
              "        1.3499756e+00, -1.4541742e+00,  1.8580415e+00, -1.3030741e+00,\n",
              "        7.9284292e-01, -1.7005992e+00,  3.2878261e+00, -8.0671114e-01,\n",
              "        6.7371144e+00, -1.1776235e+00, -1.5224946e+00, -4.1435361e-03,\n",
              "        1.6483667e+00,  1.0591946e+00, -8.0203593e-01, -3.7307553e+00,\n",
              "       -1.4754515e+00,  8.6113429e-01, -1.8348861e+00,  1.8509170e-01,\n",
              "       -1.8935969e-01,  3.7280464e+00,  1.1278170e-01, -2.4657631e+00,\n",
              "       -4.4710441e+00, -1.3550012e+00, -6.8296833e+00,  2.2718692e+00,\n",
              "        3.1013594e+00, -1.1778886e+00,  2.5311940e+00, -7.5376487e-01,\n",
              "       -3.8849249e-01, -2.4623138e-01, -7.8426450e-01,  1.2252734e+00,\n",
              "        1.5407617e+00, -1.5776865e+00,  6.4248604e-01, -2.4970851e+00,\n",
              "       -1.3874550e+00, -1.8416587e-01, -1.1900969e+00,  2.1698494e+00,\n",
              "       -2.7288496e-01, -1.1093478e+00, -1.6862392e+00, -2.7824357e+00,\n",
              "        2.4323463e+00, -1.0167981e+00, -3.7497449e+00,  5.6622308e-01,\n",
              "        3.0143213e-01, -1.8801975e+00, -3.7541342e-01,  3.8650841e-01,\n",
              "        4.8582333e-01, -3.1039944e-01, -1.8222060e+00, -2.1878328e+00,\n",
              "       -9.1262817e-02, -5.4269809e-01, -8.9775866e-01, -9.6426225e-01,\n",
              "       -1.6987110e+00,  4.4723911e+00,  3.5900624e+00,  4.4968567e+00],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    }
  ]
}