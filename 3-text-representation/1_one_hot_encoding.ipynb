{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1-one-hot-encoding.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM09pp62pS/17yqX9tPCh5p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/practical-natural-language-processing/blob/chapter-3-text-representation/1_one_hot_encoding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULjiIUimgG3y",
        "colab_type": "text"
      },
      "source": [
        "# One-Hot Encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0coq6NvgJqo",
        "colab_type": "text"
      },
      "source": [
        "In one-hot encoding, each word $w$ in the corpus vocabulary is given a unique integer ID $w_{id}$ that is between 1 and $|V|$, where $V$ is the set of the corpus vocabulary. Each word is then represented by a V-dimensional binary vector of 0s and 1s. This is done via a $|V|$ dimension vector filled with all 0s barring the index, where index = $w_{id}$. At this index, we simply put a 1. The representation for individual words is then combined to form a sentence representation.\n",
        "\n",
        "**Our toy corpus**\n",
        "\n",
        "|  |  | \n",
        "| --- | --- | \n",
        "| D1 | Dog bites man. |\n",
        "| D2 | Man bites dog. |\n",
        "| D3 | Dog eats meat. |\n",
        "| D4 | Man eats food. |\n",
        "\n",
        "Let’s understand this via our toy corpus. We first map each of the six words to unique IDs: `dog = 1, bites = 2, man = 3, meat = 4 , food = 5, eats = 6.`\n",
        "\n",
        "Let’s consider the document D1: “dog bites man”. As per the scheme, each word is a six-dimensional vector. \n",
        "\n",
        "Dog is represented as `[1 0 0 0 0 0]`, as the word “dog” is mapped to ID 1. Bites is represented as `[0 1 0 0 0 0]`, and so on and so forth. \n",
        "\n",
        "**Documents**\n",
        "\n",
        "|  |  | \n",
        "| --- | --- | \n",
        "| D1 | Dog bites man. |\n",
        "| D2 | Man bites dog. |\n",
        "| D3 | Dog eats meat. |\n",
        "| D4 | Man eats food. |\n",
        "\n",
        "Other documents in the corpus can be represented similarly. Finaly, we will get this matrix for **One-Hot Encoding**.:\n",
        "\n",
        "```\n",
        " V = {dog : 1, bites : 2, man : 3, meat : 4 , food : 5, eats : 6}\n",
        " D1 = [ \n",
        "   [1 0 0 0 0 0],\n",
        "   [0 1 0 0 0 0],\n",
        "   [0 0 1 0 0 0]\n",
        " ]\n",
        " D2 = [ \n",
        "   [1 0 0 0 0 0],\n",
        "   [0 1 0 0 0 0],\n",
        "   [0 0 1 0 0 0]\n",
        " ]\n",
        " D3 = [ \n",
        "   [1 0 0 0 0 0],\n",
        "   [0 0 0 0 0 1],\n",
        "   [0 0 0 1 0 0]\n",
        " ]\n",
        " D4 = [ \n",
        "  [0 0 1 0 0 0],\n",
        "  [0 0 0 0 0 1],\n",
        "  [0 0 0 0 1 0]\n",
        "]\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDKLJb4KocOk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0f10d84f-ace0-4fc3-ef4d-c693d7a5bcdc"
      },
      "source": [
        "documents = [\n",
        "  \"Dog bites man.\",\n",
        "  \"Man bites dog.\", \n",
        "  \"Dog eats meat.\", \n",
        "  \"Man eats food.\"\n",
        "]\n",
        "\n",
        "processed_docs = [doc.lower().replace('.', '') for doc in documents]\n",
        "processed_docs"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['dog bites man', 'man bites dog', 'dog eats meat', 'man eats food']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JTs9RnOp0yJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f17c63cd-8cfc-4ab1-8738-2ce39cc60167"
      },
      "source": [
        "# Build the vocabulary\n",
        "vocab = {}\n",
        "count = 0\n",
        "for doc in processed_docs:\n",
        "  for word in doc.split():\n",
        "    if word not in vocab:\n",
        "      count = count + 1\n",
        "      vocab[word] = count\n",
        "print(vocab)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'dog': 1, 'bites': 2, 'man': 3, 'eats': 4, 'meat': 5, 'food': 6}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvIF8MGFq3uX",
        "colab_type": "text"
      },
      "source": [
        "Let's get one hot representation for any string based on this vocabulary. \n",
        "\n",
        "* If the word exists in the vocabulary, its representation is returned. \n",
        "* If not, a list of zeroes is returned for that word. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqK27zVHqm9F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_onehot_vector(somestring):\n",
        "  onehot_encoded = []\n",
        "  for word in somestring.split():\n",
        "    temp = [0] * len(vocab)\n",
        "    if word in vocab:\n",
        "      temp[vocab[word] - 1] = 1  # -1 is to take care of the fact indexing in array starts from 0 and not 1\n",
        "    onehot_encoded.append(temp)\n",
        "  return onehot_encoded "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7lU6ZLFrnW0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "9cd6cf59-06d3-4170-9f33-78af221005f2"
      },
      "source": [
        "print(processed_docs[1])\n",
        "# one hot representation for a text from our corpus.\n",
        "get_onehot_vector(processed_docs[1])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "man bites dog\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0, 0, 1, 0, 0, 0], [0, 1, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EZsSfBUrxHZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "acc41aea-7f56-45f8-8866-2b5e807704a0"
      },
      "source": [
        "# one hot representation for a random text, using the above vocabulary\n",
        "get_onehot_vector('man and dog are good')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0, 0, 1, 0, 0, 0],\n",
              " [0, 0, 0, 0, 0, 0],\n",
              " [1, 0, 0, 0, 0, 0],\n",
              " [0, 0, 0, 0, 0, 0],\n",
              " [0, 0, 0, 0, 0, 0]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdrvAtb1sL7l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "d352b169-9385-4e14-e61f-e47bb2f51f3b"
      },
      "source": [
        "get_onehot_vector('man and man are good')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0, 0, 1, 0, 0, 0],\n",
              " [0, 0, 0, 0, 0, 0],\n",
              " [0, 0, 1, 0, 0, 0],\n",
              " [0, 0, 0, 0, 0, 0],\n",
              " [0, 0, 0, 0, 0, 0]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHXxxpK9nUe0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0b7d4a66-274b-44c2-ba9b-7372037704ad"
      },
      "source": [
        "get_onehot_vector('dog bites man')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wGiVqiNnrpO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "081911c9-45a1-45d4-f0b7-74804e41d666"
      },
      "source": [
        "one_hot_vectors = get_onehot_vector('dog bites man.man eats food')\n",
        "one_hot_vectors"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 0, 0, 0, 0, 0],\n",
              " [0, 1, 0, 0, 0, 0],\n",
              " [0, 0, 0, 0, 0, 0],\n",
              " [0, 0, 0, 1, 0, 0],\n",
              " [0, 0, 0, 0, 0, 1]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cq5aup4goQtD",
        "colab_type": "text"
      },
      "source": [
        "Let's show the one-hot-vetcor in dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOpC52UDobgs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZO6LJdcaoe47",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "outputId": "d18a8b35-a02d-4e6e-851a-d7ddb9b3b5a1"
      },
      "source": [
        "pd.DataFrame(one_hot_vectors, columns=vocab, index=['dog', 'bites', 'man', 'eats', 'food'])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>dog</th>\n",
              "      <th>bites</th>\n",
              "      <th>man</th>\n",
              "      <th>eats</th>\n",
              "      <th>meat</th>\n",
              "      <th>food</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>dog</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bites</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>man</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>eats</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>food</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       dog  bites  man  eats  meat  food\n",
              "dog      1      0    0     0     0     0\n",
              "bites    0      1    0     0     0     0\n",
              "man      0      0    0     0     0     0\n",
              "eats     0      0    0     1     0     0\n",
              "food     0      0    0     0     0     1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zt0XAJw7snnP",
        "colab_type": "text"
      },
      "source": [
        "## One-hot encoding using scikit -learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Prh8v5JSsoUc",
        "colab_type": "text"
      },
      "source": [
        "In real world projects one mostly uses scikit -learn’s implementation of one hot encoding.\n",
        "\n",
        "**We encode our corpus as a one-hot numeric array using scikit-learn's OneHotEncoder.**\n",
        "\n",
        "We will demostrate:\n",
        "* **One Hot Encoding**: In one-hot encoding, each word $w$ in corpus vocabulary is given a unique integer id $w_{id}$ that is between 1 and $|V|$, where $V$ is the set of corpus vocab. Each word is then represented by a V-dimensional binary vector of 0s and 1s.\n",
        "* **Label Encoding**: In Label Encoding, each word w in our corpus is converted into a numeric value between 0 and n-1 (where n refers to number of unique words in our corpus).\n",
        "\n",
        "Link for the official documentation of both can be found [here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) and [here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html) respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPECPNOgsV76",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "S1 = 'dog bites man'\n",
        "S2 = 'man bites dog'\n",
        "S3 = 'dog eats meat'\n",
        "S4 = 'man eats food'"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5QNpWuktoft",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "63bcf821-763c-44c1-f82d-a10ee378165c"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "\n",
        "data = [S1.split(), S2.split(), S3.split(), S4.split()]\n",
        "values = data[0] + data[1] + data[2] + data[3]\n",
        "print('The data:', values)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The data: ['dog', 'bites', 'man', 'man', 'bites', 'dog', 'dog', 'eats', 'meat', 'man', 'eats', 'food']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIvu-AwRuOjU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c18edee6-7c72-46dc-8fe3-fd6dcc6b4108"
      },
      "source": [
        "# Label Encoding\n",
        "label_encoder = LabelEncoder()\n",
        "integer_encoded = label_encoder.fit_transform(values)\n",
        "print('Label Encoded:', integer_encoded)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Label Encoded: [1 0 4 4 0 1 1 2 5 4 2 3]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J80jozC5ujrb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "8d732752-faf2-4781-cb39-543cce5945c6"
      },
      "source": [
        "# One-Hot Encoding\n",
        "onehot_encoder = OneHotEncoder()\n",
        "onehot_encoded = onehot_encoder.fit_transform(data).toarray()\n",
        "print('Onehot Encoded Matrix:\\n', onehot_encoded)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Onehot Encoded Matrix:\n",
            " [[1. 0. 1. 0. 0. 0. 1. 0.]\n",
            " [0. 1. 1. 0. 1. 0. 0. 0.]\n",
            " [1. 0. 0. 1. 0. 0. 0. 1.]\n",
            " [0. 1. 0. 1. 0. 1. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0F1546VZpUXV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "7530002b-4fed-44cb-b984-4f231f103d28"
      },
      "source": [
        "pd.DataFrame(onehot_encoded)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     0    1    2    3    4    5    6    7\n",
              "0  1.0  0.0  1.0  0.0  0.0  0.0  1.0  0.0\n",
              "1  0.0  1.0  1.0  0.0  1.0  0.0  0.0  0.0\n",
              "2  1.0  0.0  0.0  1.0  0.0  0.0  0.0  1.0\n",
              "3  0.0  1.0  0.0  1.0  0.0  1.0  0.0  0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDfXtrdWv2kT",
        "colab_type": "text"
      },
      "source": [
        "On the positive side, one-hot encoding is intuitive to understand and straightforward to implement. However, it suffers from a few shortcomings:\n",
        "\n",
        "* he size of a one-hot vector is directly proportional to size of the vocabulary, and most real-world corpora have large vocabularies. This results in a sparse representation where most of the entries in the vectors are zeroes, making it computationally inefficient to store, compute with, and learn from (sparsity leads to overfitting).\n",
        "\n",
        "* This representation does not give a fixed-length representation for text, i.e., if a text has 10 words, you get a longer representation for it as compared to a text with 5 words. For most learning algorithms, we need the feature vectors to be of the same length.\n",
        "\n",
        "* It treats words as atomic units and has no notion of (dis)similarity between words. For example, consider three words: run, ran, and apple. Run and ran have similar meanings as opposed to run and apple. But if we take their respective vectors and compute Euclidean distance between them, they’re all equally apart. Thus, semantically, they’re very poor at capturing the meaning of the word in relation to other words.\n",
        "\n",
        "* Say we train a model using our toy corpus. At runtime, we get a sentence: “man eats fruits.” The training data didn’t include “fruit”  and there’s no way to represent it in our model. This is known as the out of vocabulary (OOV) problem. A one-hot encoding scheme cannot handle this. The only way is to retrain the model: start by expanding the vocabulary, give an ID to the new word, etc.\n",
        "\n",
        "> These days, one-hot encoding scheme is seldom used.\n",
        "\n",
        "Some of these shortcomings can be addressed by the bag-of-words approach."
      ]
    }
  ]
}