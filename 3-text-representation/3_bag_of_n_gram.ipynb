{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3-bag-of-n-gram.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP+wTFDVNMcJKZ4n3GBv8dd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/practical-natural-language-processing/blob/chapter-3-text-representation/3_bag_of_n_gram.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPJvYeCmI7vy",
        "colab_type": "text"
      },
      "source": [
        "# Bag of N-Grams"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLZUxtltI8hf",
        "colab_type": "text"
      },
      "source": [
        "**All the representation schemes we’ve seen so far treat words as independent units. There is no notion of phrases or word ordering.** The bag-of-n-grams (BoN) approach tries to remedy this. It does so by breaking text into chunks of n contiguous words (or tokens). This can help us capture some context, which earlier approaches could not do. Each chunk is called an **n-gram**. \n",
        "\n",
        "The corpus vocabulary, $V$, is then nothing but a collection of all unique n-grams across the text corpus. Then, each document in the corpus is represented by a vector of length $|V|$. This vector simply contains the frequency counts of n-grams present in the document and zero for the n-grams that are not present.\n",
        "\n",
        "\n",
        "**Our toy corpus**\n",
        "\n",
        "|  |  | \n",
        "| --- | --- | \n",
        "| D1 | Dog bites man. |\n",
        "| D2 | Man bites dog. |\n",
        "| D3 | Dog eats meat. |\n",
        "| D4 | Man eats food. |\n",
        "\n",
        "To elaborate, let’s consider our example corpus. Let’s construct a 2-gram (a.k.a. bigram) model for it. The set of all bigrams in the corpus is as follows: `{dog bites, bites man, man bites, bites dog, dog eats, eats meat, man eats, eats food}`. Then, BoN representation consists of an eight-dimensional vector for each document. The bigram representation for the first two documents is as follows: `D1 : [1,1,0,0,0,0,0,0], D2 : [0,0,1,1,0,0,0,0]`. \n",
        "\n",
        "The other two documents follow similarly. Note that the BoW scheme is a special case of the BoN scheme, with n=1. n=2 is called a “bigram model,” and n=3 is called a “trigram model.” **Further, note that, by increasing the value of n, we can incorporate larger context; however, this further increases the sparsity.** In NLP parlance, the BoN scheme is also called “n-gram feature selection.”\n",
        "\n",
        "Finaly, we will get this matrix for **Bag-of-N-Grams**.\n",
        "\n",
        "**Documents**\n",
        "\n",
        "|  |  | \n",
        "| --- | --- | \n",
        "| D1 | Dog bites man. |\n",
        "| D2 | Man bites dog. |\n",
        "| D3 | Dog eats meat. |\n",
        "| D4 | Man eats food. |\n",
        "\n",
        "**Bag-of-bi-gram Matrix**\n",
        "\n",
        "|   | dog bites | bites man | man bites | bites dog | dog eats | eats meat | man eats | eats food | \n",
        "| --- | --- | --- | --- | --- | --- | --- | --- | --- |\n",
        "| D1 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | \n",
        "| D2 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | \n",
        "| D3 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | \n",
        "| D4 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 |"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53SIYnGLLJGe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "fdc0f236-6460-4b0e-a80e-51b5fb1f39c9"
      },
      "source": [
        "documents = [\n",
        "  \"Dog bites man.\",\n",
        "  \"Man bites dog.\", \n",
        "  \"Dog eats meat.\", \n",
        "  \"Man eats food.\"\n",
        "]\n",
        "\n",
        "processed_docs = [doc.lower().replace('.', '') for doc in documents]\n",
        "processed_docs"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['dog bites man', 'man bites dog', 'dog eats meat', 'man eats food']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4f9IgUuPVFq",
        "colab_type": "text"
      },
      "source": [
        "Now, let's do the main task of finding bag of n-gram representation. We will use CountVectorizer from sklearn for a BoN representation considering 1–3 n-gram word features to represent the corpus that we’ve used so far. Here, we use unigram, bigram, and trigram vectors by setting `ngram_range = (1,3)`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8udT5HgiPSZq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Ngram vectorization example with count vectorizer and uni, bi, trigrams\n",
        "count_vect = CountVectorizer(ngram_range=(1, 2))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXaVVKmwPnOc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "0a35086d-6fa9-45fe-caec-dfabc88dfc53"
      },
      "source": [
        "# Build a BOW representation for the corpus\n",
        "bow_rep = count_vect.fit_transform(processed_docs)\n",
        "print('Our vocabulary: ', count_vect.vocabulary_)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Our vocabulary:  {'dog': 3, 'bites': 0, 'man': 10, 'dog bites': 4, 'bites man': 2, 'man bites': 11, 'bites dog': 1, 'eats': 6, 'meat': 13, 'dog eats': 5, 'eats meat': 8, 'food': 9, 'man eats': 12, 'eats food': 7}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jo-WDqzwQDh8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "12d500d8-9134-41d2-f185-ececd9ea04a2"
      },
      "source": [
        "print(\"BoW representation for 'dog bites man': \", bow_rep[0].toarray())\n",
        "print(\"BoW representation for 'man bites dog': \", bow_rep[1].toarray())"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BoW representation for 'dog bites man':  [[1 0 1 1 1 0 0 0 0 0 1 0 0 0]]\n",
            "BoW representation for 'man bites dog':  [[1 1 0 1 0 0 0 0 0 0 1 1 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3_rT-zzQ7t5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "e37b7c30-ec2a-43f0-fffa-6314768c5363"
      },
      "source": [
        "bow_rep.toarray()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
              "       [1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0],\n",
              "       [0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1],\n",
              "       [0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oazSaHXtQ_FL",
        "colab_type": "text"
      },
      "source": [
        "Let's show the Bag-of-N-Gram vetcors in dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLNTwaSpRMxT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "9b10e081-2a9c-486a-dc09-ffa239b0da6a"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "bow_cols = [key for key, _ in count_vect.vocabulary_.items()]\n",
        "bow_indexs = ['D1', 'D2', 'D3', 'D4']\n",
        "pd.DataFrame(bow_rep.toarray(), columns=bow_cols, index=bow_indexs)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>dog</th>\n",
              "      <th>bites</th>\n",
              "      <th>man</th>\n",
              "      <th>dog bites</th>\n",
              "      <th>bites man</th>\n",
              "      <th>man bites</th>\n",
              "      <th>bites dog</th>\n",
              "      <th>eats</th>\n",
              "      <th>meat</th>\n",
              "      <th>dog eats</th>\n",
              "      <th>eats meat</th>\n",
              "      <th>food</th>\n",
              "      <th>man eats</th>\n",
              "      <th>eats food</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>D1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>D2</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>D3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>D4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    dog  bites  man  dog bites  ...  eats meat  food  man eats  eats food\n",
              "D1    1      0    1          1  ...          1     0         0          0\n",
              "D2    1      1    0          1  ...          1     1         0          0\n",
              "D3    0      0    0          1  ...          0     0         0          1\n",
              "D4    0      0    0          0  ...          1     0         1          0\n",
              "\n",
              "[4 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsDsbGxqQXxP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3e52fda6-485b-4730-ce4d-de5771461b1c"
      },
      "source": [
        "# Get the representation using this vocabulary, for a new text\n",
        "temp = count_vect.transform(['dog and dog are friends'])\n",
        "print(\"Bow representation for 'dog and dog are friends':\", temp.toarray())"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Bow representation for 'dog and dog are friends': [[0 0 0 2 0 0 0 0 0 0 0 0 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eA5Qn3NSRt8O",
        "colab_type": "text"
      },
      "source": [
        "Here are the main pros and cons of BoN:\n",
        "\n",
        "* It captures some context and word-order information in the form of n-grams.\n",
        "* Thus, resulting vector space is able to capture some semantic similarity. Documents having the same n-grams will have their vectors closer to each other in Euclidean space as compared to documents with completely different n-grams.\n",
        "* As n increases, dimensionality (and therefore sparsity) only increases rapidly.\n",
        "* It still provides no way to address the OOV problem."
      ]
    }
  ]
}