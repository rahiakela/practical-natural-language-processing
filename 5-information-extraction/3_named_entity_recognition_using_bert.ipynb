{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3-named-entity-recognition-using-bert.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMGmjHP3vwExXa8h4P9+eo/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/practical-natural-language-processing/blob/chapter-5-information-extraction/3_named_entity_recognition_using_bert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wkxG9WsGxOW"
      },
      "source": [
        "# Named Entity Recognition using BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WimeNGEKHB5G"
      },
      "source": [
        "Consider a scenario where the user asks a search query—“Where was Albert Einstein born?”—using Google search.\r\n",
        "\r\n",
        "<img src='https://github.com/practical-nlp/practical-nlp-figures/raw/master/figures/5-5.png?raw=1' width='800'/>\r\n",
        "\r\n",
        "To be able to show “Ulm, Germany” for this query, the search engine needs to decipher that Albert Einstein is a person before going on to look for a place of birth. This is an example of NER in action in a real-world application.\r\n",
        "\r\n",
        "**NER refers to the IE task of identifying the entities in a document. Entities are typically names of persons, locations, and organizations, and other specialized strings, such as money expressions, dates, products, names/numbers of laws or articles, and so on. NER is an important step in the pipeline of several NLP applications involving information extraction.**\r\n",
        "\r\n",
        "<img src='https://github.com/practical-nlp/practical-nlp-figures/raw/master/figures/5-6.png?raw=1' width='800'/>\r\n",
        "\r\n",
        "As seen in the figure, for a given text, NER is expected to identify person names, locations, dates, and other entities. Different categories of entities identified here are some of the ones commonly used in NER system development.\r\n",
        "\r\n",
        "**NER is a prerequisite for being able to do other IE tasks, such as relation extraction or event extraction**.\r\n",
        "\r\n",
        "NER is also useful in other applications like machine translation, as names\r\n",
        "need not necessarily be translated while translating a sentence. So, clearly, there’s a range of scenarios in NLP projects where NER is a major component. It’s one of the common tasks you’re likely to encounter in NLP projects in industry."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "334d5niWI4Kb"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKh4N4O2dYDR"
      },
      "source": [
        "%tensorflow_version 1.x\r\n",
        "!pip install pytorch-pretrained-bert==0.4.0\r\n",
        "!pip install seqeval==0.0.12"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3k9pgNBI5aJ",
        "outputId": "fe2ffeef-f9a8-4d2c-b3aa-3ee74930f67c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# importing packages for string processing,dataframe handling, array manipulations, etc\r\n",
        "import string\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "from tqdm import tqdm, trange\r\n",
        "\r\n",
        "# importing all the pytorch packages\r\n",
        "import torch\r\n",
        "from torch.optim import Adam\r\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\r\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertConfig\r\n",
        "from pytorch_pretrained_bert import BertForTokenClassification, BertAdam\r\n",
        "\r\n",
        "# importing additonal packages to aid preprocessing of data\r\n",
        "from keras.preprocessing.sequence import pad_sequences\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "# importing packages to calculate the f1_score of our model\r\n",
        "from seqeval.metrics import f1_score"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOSKkHvBIxVc"
      },
      "source": [
        "## Building an NER System"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3-nEGIwIyXT"
      },
      "source": [
        "A simple approach to building an NER system is to maintain a large collection of person/ organization/location names that are the most relevant to our company (e.g., names of all clients, cities in their addresses, etc.); this is typically referred to as a gazetteer. To check whether a given word is a named entity or not, just do a lookup in the gazetteer. If a large number of entities present in our data are covered by a gazetteer, then it’s a great way to start, especially when we don’t have an existing NER system available.\r\n",
        "\r\n",
        "An approach that goes beyond a lookup table is rule-based NER, which can be based on a compiled list of patterns based on word tokens and POS tags.\r\n",
        "\r\n",
        "For example, a pattern “NNP was born,” where “NNP” is the POS tag for a proper noun, indicates that the word that was tagged “NNP” refers to a person. Such rules can be programmed to cover as many cases as possible to build a rule-based NER system. \r\n",
        "\r\n",
        "1. **[Stanford NLP’s RegexNER](https://nlp.stanford.edu/software/regexner.html)**\r\n",
        "2. **[spaCy’s EntityRuler](https://spacy.io/usage/rule-based-matching#entityruler)**\r\n",
        "\r\n",
        "provide functionalities to implement your own rule-based NER.\r\n",
        "\r\n",
        "A more practical approach to NER is to train an ML model, which can predict the\r\n",
        "named entities in unseen text. For each word, a decision has to be made whether or not that word is an entity, and if it is, what type of the entity it is. In many ways, this is very similar to the classification problems.\r\n",
        "\r\n",
        "**The only difference here is that NER is a “sequence labeling” problem.**\r\n",
        "\r\n",
        "The typical classifiers predict labels for texts independent of their surrounding context. Consider a classifier that classifies sentences in a movie review into positive/negative/neutral categories based on their sentiment. This classifier does not (usually) take into account the sentiment of previous (or subsequent) sentences when classifying the current sentence.\r\n",
        "\r\n",
        "**In a sequence classifier, such context is important. A common use case for sequence labeling is POS tagging, where we need information about the parts of speech of surrounding words to estimate the part of speech of the current word. NER is traditionally modeled as a sequence classification problem, where the entity prediction for the current word also depends on the context.**\r\n",
        "\r\n",
        "For example, if the previous word was a person name, there’s a higher probability that the current word is also a person name if it’s a noun (e.g., first and last names).\r\n",
        "\r\n",
        "To illustrate the difference between a normal classifier and a sequence classifier, consider the following sentence: “Washington is a rainy state.” When a normal classifier sees this sentence and has to classify it word by word, it has to make a decision as to whether Washington refers to a person (e.g., George Washington) or the State of Washington without looking at the surrounding words. It’s possible to classify the word “Washington” in this particular sentence as a location only after looking at the context in which it’s being used. It’s for this reason that sequence classifiers are used\r\n",
        "for training NER models.\r\n",
        "\r\n",
        "**Conditional random fields (CRFs) is one of the popular sequence classifier training algorithms.**\r\n",
        "\r\n",
        "Recent advances in NER research either exclude or augment the kind of feature engineering we did in this example with neural network models. [NCRF++](https://github.com/jiesutd/NCRFpp) is another library that can be used to train your own NER using different neural network architectures. This notebook that uses the BERT model for training an NER system using the same dataset.\r\n",
        "\r\n",
        "NCRF++ is a PyTorch based framework with flexiable choices of input features and output structures. The design of neural sequence labeling models with NCRF++ is fully configurable through a configuration file, which does not require any code work. NCRF++ can be regarded as a neural network version of CRF++, which is a famous statistical CRF framework.\r\n",
        "\r\n",
        "To perform sequence classification, we need data in a format that allows us to model the context. Typical training data for NER looks like below, which is a sentence from the CONLL-03 dataset.\r\n",
        "\r\n",
        "\r\n",
        "<img src='https://github.com/practical-nlp/practical-nlp-figures/raw/master/figures/5-7.png?raw=1' width='800'/>\r\n",
        "\r\n",
        "The labels in the figure follow what’s known as a BIO notation: B indicates the beginning of an entity; I, inside an entity, indicates when entities comprise more than one word; and O, other, indicates non-entities. Peter Such is a name with two words in the example shown above.\r\n",
        "\r\n",
        "Thus, “Peter” gets tagged as a B-PER, and “Such” gets tagged as an I-PER to indicate that Such is a part of the entity from the previous word. The remaining entities in this example, Essex, Yorkshire, and Headingley, are\r\n",
        "all one-word entities. So, we only see B-ORG and B-LOC as their tags. Once we\r\n",
        "obtain a dataset of sentences annotated in this form and we have a sequence classifier algorithm, how should we train an NER system?\r\n",
        "\r\n",
        "The steps are the same as those for the text classifiers:\r\n",
        "1. Load the dataset\r\n",
        "2. Extract the features\r\n",
        "3. Train the classifier\r\n",
        "4. Evaluate it on a test set\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "po6Tg7YqcKMA"
      },
      "source": [
        "### Loading The Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AC-azFzEcMLd"
      },
      "source": [
        "Loading the dataset is straightforward. This particular dataset is also already split into a train/dev/test set. So, we’ll train the model using the training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UGRGpa4-gVBn",
        "outputId": "1ccd377e-bbc4-4511-d38f-3a0ab1970908"
      },
      "source": [
        "%%shell\r\n",
        "\r\n",
        "wget https://raw.githubusercontent.com/practical-nlp/practical-nlp/master/Ch5/Data/conlldata/test.txt\r\n",
        "wget https://raw.githubusercontent.com/practical-nlp/practical-nlp/master/Ch5/Data/conlldata/train.txt\r\n",
        "\r\n",
        "mkdir conlldata\r\n",
        "mv *.txt conlldata"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-12-31 09:47:52--  https://raw.githubusercontent.com/practical-nlp/practical-nlp/master/Ch5/Data/conlldata/test.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 376236 (367K) [text/plain]\n",
            "Saving to: ‘test.txt’\n",
            "\n",
            "\rtest.txt              0%[                    ]       0  --.-KB/s               \rtest.txt            100%[===================>] 367.42K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2020-12-31 09:47:52 (15.5 MB/s) - ‘test.txt’ saved [376236/376236]\n",
            "\n",
            "--2020-12-31 09:47:52--  https://raw.githubusercontent.com/practical-nlp/practical-nlp/master/Ch5/Data/conlldata/train.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1655711 (1.6M) [text/plain]\n",
            "Saving to: ‘train.txt’\n",
            "\n",
            "train.txt           100%[===================>]   1.58M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2020-12-31 09:47:53 (24.1 MB/s) - ‘train.txt’ saved [1655711/1655711]\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9s7dAiLkcD5h"
      },
      "source": [
        "\"\"\"\r\n",
        "Load the training/testing data. \r\n",
        "input: conll format data, but with only 2 tab separated colums - words and NEtags.\r\n",
        "output: A list where each item is 2 lists.  sentence as a list of tokens, NER tags as a list for each token.\r\n",
        "\"\"\"\r\n",
        "def load__data_conll(file_path):\r\n",
        "  myoutput, words, tags = [], [], []\r\n",
        "  fh = open(file_path)\r\n",
        "  for line in fh:\r\n",
        "    line = line.strip()\r\n",
        "    if \"\\t\" not in line:\r\n",
        "      # Sentence ended.\r\n",
        "      myoutput.append([words, tags])\r\n",
        "      words, tags = [], []\r\n",
        "    else:\r\n",
        "      word, tag = line.split(\"\\t\")\r\n",
        "      words.append(word)\r\n",
        "      tags.append(tag)\r\n",
        "      \r\n",
        "  fh.close()\r\n",
        "  return myoutput"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxjVdx6bRunz"
      },
      "source": [
        "### Extract the features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOlgJYKvL-Vi"
      },
      "source": [
        "Let’s look at an example using handcrafted features this time. What features seem intuitively relevant for this task? To identify names of people or places, for example, patterns such as whether the word starts with an uppercase character or whether it’s preceded or succeeded by a verb/ noun, etc., can be used as starting points to train an NER model. \r\n",
        "\r\n",
        "The following function that extracts the previous and next words’ POS tags for a given sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWrM_IteMIfB"
      },
      "source": [
        "\"\"\"\r\n",
        "Get features for all words in the sentence\r\n",
        "Features:\r\n",
        "- word context: a window of 2 words on either side of the current word, and current tag.\r\n",
        "- POS context: a window of 2 POS tags on either side of the current word, and current tag. \r\n",
        "input: sentence as a list of tokens.\r\n",
        "output: list of dictionaries. each dict represents features for that word.\r\n",
        "\"\"\"\r\n",
        "def sent2features(sentence):\r\n",
        "  features = []\r\n",
        "  sent_tags = pos_tag(sentence)   # This format is specific to this POS tagger!\r\n",
        "  for i in range(0, len(sentence)):\r\n",
        "    word = sentence[i]\r\n",
        "    wordfeatures = {}\r\n",
        "    # word features: word, prev 2 words, next 2 words in the sentence.\r\n",
        "    wordfeatures[\"word\"] = word\r\n",
        "\r\n",
        "    if i == 0:\r\n",
        "      wordfeatures[\"prevWord\"] = wordfeatures[\"prevSecondWord\"] = \"<S>\"\r\n",
        "    elif i == 1:\r\n",
        "      wordfeatures[\"prevWord\"] = sentence[0]\r\n",
        "      wordfeatures[\"prevSecondWord\"] = \"</S>\"\r\n",
        "    else:\r\n",
        "      wordfeatures[\"prevWord\"] = sentence[i - 1]\r\n",
        "      wordfeatures[\"prevSecondWord\"] = sentence[i - 2]\r\n",
        "\r\n",
        "    # next two words as features\r\n",
        "    if i == len(sentence) - 2:\r\n",
        "      wordfeatures[\"nextWord\"] = sentence[i + 1]\r\n",
        "      wordfeatures[\"nextNextWord\"] = \"</S>\"\r\n",
        "    elif i == len(sentence) - 1:\r\n",
        "      wordfeatures[\"nextWord\"] = \"</S>\"\r\n",
        "      wordfeatures[\"nextNextWord\"] = \"</S>\"\r\n",
        "    else:\r\n",
        "      wordfeatures[\"nextWord\"] = sentence[i + 1]\r\n",
        "      wordfeatures[\"nextNextWord\"] = sentence[i + 2]\r\n",
        "\r\n",
        "    # POS tag features: current tag, previous and next 2 tags.\r\n",
        "    wordfeatures[\"tag\"] = sent_tags[i][1]\r\n",
        "    if i == 0:\r\n",
        "      wordfeatures[\"prevTag\"] = wordfeatures[\"prevSecondTag\"] = \"<S>\"\r\n",
        "    elif i == 1:\r\n",
        "      wordfeatures[\"prevTag\"] = sent_tags[0][1]\r\n",
        "      wordfeatures[\"prevSecondTag\"] = \"</S>\"\r\n",
        "    else:\r\n",
        "      wordfeatures[\"prevTag\"] = sent_tags[i -1][1]\r\n",
        "      wordfeatures[\"prevSecondTag\"] = sent_tags[i - 2][1]\r\n",
        "\r\n",
        "    # next two words as features\r\n",
        "    if i == len(sentence) - 2:\r\n",
        "      wordfeatures[\"nextTag\"] = sent_tags[i + 1][1]\r\n",
        "      wordfeatures[\"nextNextTag\"] = \"</S>\"\r\n",
        "    elif i == len(sentence) - 1:\r\n",
        "      wordfeatures[\"nextTag\"] = \"</S>\"\r\n",
        "      wordfeatures[\"nextNextTag\"] = \"</S>\"\r\n",
        "    else:\r\n",
        "      wordfeatures[\"nextTag\"] = sent_tags[i + 1][1]\r\n",
        "      wordfeatures[\"nextNextTag\"] = sent_tags[i + 2][1]\r\n",
        "    \r\n",
        "    # That is it! You can add whatever you want!\r\n",
        "    features.append(wordfeatures)\r\n",
        "  return features"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93TG87jISuIr"
      },
      "source": [
        "# preprocess the data by calling the functions\r\n",
        "train_path = 'conlldata/train.txt'\r\n",
        "test_path = 'conlldata/test.txt'\r\n",
        "\r\n",
        "conll_train = load__data_conll(train_path)\r\n",
        "conll_test = load__data_conll(test_path)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7zbwapNSi77"
      },
      "source": [
        "### Pre-process the text data according to BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjeKRAwiSlXg"
      },
      "source": [
        "BERT needs us to pre-process the data in a particular way.\r\n",
        "\r\n",
        "Lets take the raw data from the txt files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7J1aa_TlRRFg",
        "outputId": "758c0538-192d-4a76-c263-0d858831cc5e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df_train = pd.read_csv(\"conlldata/train.txt\", engine=\"python\", delimiter=\"\\t\", header=None, encoding='utf-8', error_bad_lines=False)\r\n",
        "df_test = pd.read_csv(\"conlldata/test.txt\", engine=\"python\", delimiter=\"\\t\", header=None, encoding='utf-8', error_bad_lines=False)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Skipping line 23407: unexpected end of data\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86VDlSOYSQlX"
      },
      "source": [
        "# merge dataframe\r\n",
        "df = pd.merge(df_train, df_test)\r\n",
        "\r\n",
        "# we will be using this to make a set of all unique labels\r\n",
        "label = list(df[1].values)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zW2MfkxRXjSZ",
        "outputId": "f158eeae-d711-4a3f-f7ee-cecb8109637b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# calculating the size\r\n",
        "np.array(conll_train).shape"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(14041, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqPe9Dd-PQ3x",
        "outputId": "12fd29f4-1746-4807-ba68-f12cd8642f41",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "np.array(conll_test).shape"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3453, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvT2iaErYE4L"
      },
      "source": [
        "We need to join all the tokens into a single sentence. We will use the untokenize function in token_utils from [this github repo](https://github.com/commonsense/metanl/blob/master/metanl/token_utils.py)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dOdyopIiXn0Z",
        "outputId": "3a9f18fe-a2c8-46fd-de1d-64086d7ea327"
      },
      "source": [
        "if __name__ == \"__main__\":\r\n",
        "  train_path = \"conlldata/train.txt\"\r\n",
        "  test_path = \"conlldata/test.txt\"\r\n",
        "\r\n",
        "  # 1. Load the dataset\r\n",
        "  conll_train = load__data_conll(train_path)\r\n",
        "  conll_dev = load__data_conll(test_path)\r\n",
        "\r\n",
        "  # 2. Extract the features\r\n",
        "  print(\"Training a Sequence classification model with CRF\")\r\n",
        "  features, labels = get_features_conll(conll_train)\r\n",
        "  devfeatures, devlabels = get_features_conll(conll_dev)\r\n",
        "  #print(features.shape, labels.shape)\r\n",
        "  #print(devfeatures.shape, devlabels.shape)\r\n",
        "\r\n",
        "  # 3. Train the classifier\r\n",
        "  train_sequence(features, labels, devfeatures, devlabels)\r\n",
        "  print(\"Done with sequence model\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training a Sequence classification model with CRF\n",
            "0.9255103670420659\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           O      0.973     0.981     0.977     38323\n",
            "       B-LOC      0.694     0.765     0.728      1668\n",
            "       I-LOC      0.738     0.482     0.584       257\n",
            "      B-MISC      0.648     0.309     0.419       702\n",
            "      I-MISC      0.626     0.505     0.559       216\n",
            "       B-ORG      0.670     0.561     0.611      1661\n",
            "       I-ORG      0.551     0.704     0.618       835\n",
            "       B-PER      0.773     0.766     0.769      1617\n",
            "       I-PER      0.819     0.886     0.851      1156\n",
            "\n",
            "    accuracy                          0.928     46435\n",
            "   macro avg      0.721     0.662     0.679     46435\n",
            "weighted avg      0.926     0.928     0.926     46435\n",
            "\n",
            "\n",
            "\n",
            "                O  B-LOC  I-LOC B-MISC I-MISC  B-ORG  I-ORG  B-PER  I-PER \n",
            "         O  37579    118      3     22     32    193    224     88     64 38323\n",
            "     B-LOC    143   1276      1     36      1     95     14     98      4 1668\n",
            "     I-LOC     32      6    124      1      5      0     52      0     37 257\n",
            "    B-MISC    344     48      1    217      2     56     13     19      2 702\n",
            "    I-MISC     58      1      4      4    109      2     29      0      9 216\n",
            "     B-ORG    265    236      0     48      3    932     20    151      6 1661\n",
            "     I-ORG     76     15     18      2     15     21    588      8     92 835\n",
            "     B-PER     86    138      1      5      3     90     44   1238     12 1617\n",
            "     I-PER     26      1     16      0      4      2     83      0   1024 1156\n",
            "Done with sequence model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hw1E8PomhjE5"
      },
      "source": [
        "Training this CRF model gave an F1 score of 0.92 on the development data, which is a very good score! \r\n",
        "\r\n",
        "Here, we showed some of the most commonly used features in learning an NER system and used a popular training method and a publicly available dataset.\r\n",
        "\r\n",
        "Clearly, there’s a lot to be done in terms of tuning the model and developing\r\n",
        "(even) better features; this example only serves to illustrate one way of developing an NER model quickly using one particular library. [MITIE](https://github.com/mit-nlp/MITIE) is another such library to train NER systems."
      ]
    }
  ]
}