{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2-effect-of-different-tokenizers-on-smtd.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOGSnxGdAz0hMS3eCo0ceLS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/practical-natural-language-processing/blob/master/8-social-media/2_effect_of_different_tokenizers_on_smtd.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLxpKLlDQSyg"
      },
      "source": [
        "## Effect of different tokenizers on Social Media Text Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbPLv0rER3pD"
      },
      "source": [
        "One of the key steps in the above process is to correctly tokenize the text data. For this, we used twokenize to get tokens from the text corpus. This is a specialized function for getting tokens from tweetsâ€™ text data. This function is part of a set of NLP tools specially designed to work with SMTD. \n",
        "\n",
        "Now, we might ask: why do we need a specialized tokenizer, and why not use the standard tokenizer available in NLTK?\n",
        "\n",
        "The answer lies in the fact that the tokenizer available in NLTK is designed to\n",
        "work with standard English language. Specifically in the English language, two words are separated by space. This might not necessarily be true for English used on Twitter.\n",
        "\n",
        "This suggests that a tokenizer that uses space as a way to identify word boundaries might not do well on SMTD. Letâ€™s understand this with an example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqMbpeFvSgAL"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJ8zKNqsShNh"
      },
      "source": [
        "!pip install twikenizer\n",
        "!pip install emoji"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3W3-EXM9TZhO"
      },
      "source": [
        "import twikenizer as twk\n",
        "from nltk.tokenize import TweetTokenizer, word_tokenize\n",
        "import nltk"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S93shAtPnU6I",
        "outputId": "e9ff5435-9a9e-465d-a4f3-e596dfa536a7"
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cmy2WXLXSx01"
      },
      "source": [
        "## Tweet tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Byx2J_KNS4LE"
      },
      "source": [
        "Consider the following tweet:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0EWD7m7OSmot",
        "outputId": "2d525f15-b139-4ac0-cd0d-f7f3b69df1be"
      },
      "source": [
        "tweet1 = \"Hey @NLPer! This is a #NLProc tweet :-D\"\n",
        "\n",
        "twk = twk.Twikenizer()\n",
        "print(twk.tokenize(tweet1))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Hey', '@NLPer', '!', 'This', 'is', 'a', '#NLProc', 'tweet', ':', '-', 'D']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxZZUiGinJ9a"
      },
      "source": [
        "Using a tokenizer designed for the English language, like nltk.tokenize.word_tokenize, weâ€™ll get the following tokens:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7NsZD9Sjmzub",
        "outputId": "89391ff8-2296-4054-b9e5-4ddd507ca8f7"
      },
      "source": [
        "print(word_tokenize(tweet1))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Hey', '@', 'NLPer', '!', 'This', 'is', 'a', '#', 'NLProc', 'tweet', ':', '-D']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhcPmWslnhUV"
      },
      "source": [
        "Clearly, the set of tokens given by the tokenizer in NLTK is not correct. Itâ€™s\n",
        "important to use a tokenizer that gives correct tokens. twokenize is specifically designed to deal with SMTD.\n",
        "\n",
        "Once we have the correct set of tokens, frequency counting is straightforward. There are a number of specialized tokenizers available to work with SMTD."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jaqm_nmZnRxD"
      },
      "source": [
        "tweet2 = 'Tw33t a_!aa&!a?b #%lol # @dude_really #b3st_day $ad (b@e) (beep#d) @dude. ðŸ˜€ðŸ˜€ !ðŸ˜€abc %ðŸ˜€lol #loveit #love.it $%&/ d*ck-'"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5db7adp5oyJ0",
        "outputId": "a12b3e84-c431-4923-e878-16a008d70451"
      },
      "source": [
        "twk = twk.Twikenizer()\n",
        "print(twk.tokenize(tweet2))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Tw33t', 'a', '_', '!', 'aa', '&', '!', 'a', '?', 'b', '#%lol', '#', '@dude_really', '#b3st_day', '$ad', '(', 'b', '@', 'e', ')', '(', 'beep', '#', 'd', ')', '@dude', '.', 'ðŸ˜€', 'ðŸ˜€', '!', 'ðŸ˜€', 'abc', '%', 'ðŸ˜€', 'lol', '#loveit', '#love', '.', 'it', '$', '%', '&', '/', 'd*ck', '-']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MU_ni6bbo4XG",
        "outputId": "063a59e4-143c-4a58-f4ad-3861a0475beb"
      },
      "source": [
        "tknzr = TweetTokenizer()\n",
        "print(tknzr.tokenize(tweet2))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Tw33t', 'a_', '!', 'aa', '&', '!', 'a', '?', 'b', '#', '%', 'lol', '#', '@dude_really', '#b3st_day', '$', 'ad', '(', 'b', '@e', ')', '(', 'beep', '#', 'd', ')', '@dude', '.', 'ðŸ˜€', 'ðŸ˜€', '!', 'ðŸ˜€', 'abc', '%', 'ðŸ˜€', 'lol', '#loveit', '#love', '.', 'it', '$', '%', '&', '/', 'd', '*', 'ck', '-']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}