{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5-deep-learning-for-text-classification.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNLKfXfrCQuwgHhqXJwAW9X",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/practical-natural-language-processing/blob/chapter-4-text-classification/5_deep_learning_for_text_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yU5hr_OBwDLK"
      },
      "source": [
        "# Deep Learning for Text Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vt5vf7J0wEDu"
      },
      "source": [
        "Deep learning is a family of machine learning algorithms where the learning happens through different kinds of multilayered neural network architectures. Over the past few years, it has shown remarkable improvements on standard machine learning tasks, such as:-\n",
        "\n",
        "- Image classification, \n",
        "- Speech recognition,\n",
        "- Machine translation. \n",
        "\n",
        "This has resulted in widespread interest in using deep learning for various tasks, including text classification. So far, we’ve seen how to train different machine learning classifiers, using BoW and different kinds of embedding\n",
        "representations. \n",
        "\n",
        "Now, let’s look at how to use deep learning architectures for text\n",
        "classification.\n",
        "\n",
        "Two of the most commonly used neural network architectures for text classification are:-\n",
        "\n",
        "- Convolutional neural networks (CNNs)\n",
        "- Recurrent neural networks (RNNs).\n",
        "\n",
        "Long short-term memory (LSTM) networks are a popular form of RNNs. Recent\n",
        "approaches also involve starting with large, pre-trained language models and finetuning them for the task at hand. \n",
        "\n",
        "In this notebook, we’ll learn how to train CNNs and LSTMs and how to tune a pre-trained language model for text classification using the IMDB sentiment classification dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdRSnbYRygYM"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8y3skLHyhx2"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.layers import Dense, Input, GlobalMaxPooling1D\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Embedding, LSTM\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.initializers import Constant\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMP_JhQWzi1Z"
      },
      "source": [
        "Here we set all the paths of all the external datasets and models such as [glove](https://nlp.stanford.edu/projects/glove/) and [IMDB reviews dataset](http://ai.stanford.edu/~amaas/data/sentiment/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CHbAody0oNu",
        "outputId": "123ddcce-3d40-45f7-cef7-1e9d1dc1f24e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-11-06 09:44:14--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2020-11-06 09:44:14--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2020-11-06 09:44:14--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  2.06MB/s    in 6m 28s  \n",
            "\n",
            "2020-11-06 09:50:43 (2.12 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "--2020-11-06 09:50:43--  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
            "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 84125825 (80M) [application/x-gzip]\n",
            "Saving to: ‘aclImdb_v1.tar.gz’\n",
            "\n",
            "aclImdb_v1.tar.gz   100%[===================>]  80.23M  22.9MB/s    in 4.9s    \n",
            "\n",
            "2020-11-06 09:50:48 (16.4 MB/s) - ‘aclImdb_v1.tar.gz’ saved [84125825/84125825]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-EDhQglK3tI_"
      },
      "source": [
        "# unzip glove file\n",
        "!unzip glove.6B.zip\n",
        "# untar IMDB dataset\n",
        "!tar xvzf aclImdb_v1.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VvUfKg24T6M",
        "outputId": "4f22138f-98b8-4cf9-c113-ca2d94bacb23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# copy to data directory\n",
        "!mkdir data\n",
        "!mv aclImdb  data"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘data’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "213lAHqh8TOa"
      },
      "source": [
        "# delete \"unsup\" directory\n",
        "!rm -rf data/aclImdb/train/unsup"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjFBjz12qWuB"
      },
      "source": [
        "# copy glove file \n",
        "!mkdir data/glove.6B\n",
        "!mv *.txt data/glove.6B"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgLJa9nMzF7e"
      },
      "source": [
        "BASE_DIR = 'data'                                        #change this to your local folder with these below datasets\n",
        "GLOVE_DIR = os.path.join(BASE_DIR, 'glove.6B')           #source: https://nlp.stanford.edu/projects/glove/\n",
        "TRAIN_DATA_DIR = os.path.join(BASE_DIR, 'aclImdb/train') #source: http://ai.stanford.edu/~amaas/data/sentiment/\n",
        "TEST_DATA_DIR = os.path.join(BASE_DIR, 'aclImdb/test') \n",
        "\n",
        "# Within these, I only have a pos/ and a neg/ folder containing text files\n",
        "MAX_SEQUENCE_LENGTH = 1000\n",
        "MAX_NUM_WORDS = 20000 \n",
        "EMBEDDING_DIM = 100 \n",
        "VALIDATION_SPLIT = 0.2"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXSjCHLi430E"
      },
      "source": [
        "#  load the data from the dataset into the notebook. Will be called twice - for train and test.\n",
        "def get_data(data_dir):\n",
        "  # list of text samples\n",
        "  texts = []\n",
        "  # dictionary mapping label name to numeric id\n",
        "  labels_index = {\"pos\": 1, \"neg\": 0}\n",
        "  # list of label ids\n",
        "  labels = []\n",
        "\n",
        "  for name in sorted(os.listdir(data_dir)):\n",
        "    path = os.path.join(data_dir, name)\n",
        "    if os.path.isdir(path):\n",
        "      label_id = labels_index[name]\n",
        "      for fname in sorted(os.listdir(path)):\n",
        "        fpath = os.path.join(path, fname)\n",
        "        text = open(fpath).read()\n",
        "        texts.append(text)\n",
        "        labels.append(label_id)\n",
        "  return texts, labels"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "deS__r_L6Vu-",
        "outputId": "d6cca096-0bd3-4089-b1f0-2ab3c5e25078",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train_texts, train_labels = get_data(TRAIN_DATA_DIR)\n",
        "test_texts, test_labels = get_data(TEST_DATA_DIR)\n",
        "\n",
        "labels_index = {\"pos\": 1, \"neg\": 0}\n",
        "\n",
        "#Just to see how the data looks like. \n",
        "print(train_texts[0])\n",
        "print(train_labels[0])\n",
        "print(test_texts[24999])\n",
        "print(test_labels[24999])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Story of a man who has unnatural feelings for a pig. Starts out with a opening scene that is a terrific example of absurd comedy. A formal orchestra audience is turned into an insane, violent mob by the crazy chantings of it's singers. Unfortunately it stays absurd the WHOLE time with no general narrative eventually making it just too off putting. Even those from the era should be turned off. The cryptic dialogue would make Shakespeare seem easy to a third grader. On a technical level it's better than you might think with some good cinematography by future great Vilmos Zsigmond. Future stars Sally Kirkland and Frederic Forrest can be seen briefly.\n",
            "0\n",
            "I've seen this story before but my kids haven't. Boy with troubled past joins military, faces his past, falls in love and becomes a man. The mentor this time is played perfectly by Kevin Costner; An ordinary man with common everyday problems who lives an extraordinary conviction, to save lives. After losing his team he takes a teaching position training the next generation of heroes. The young troubled recruit is played by Kutcher. While his scenes with the local love interest are a tad stiff and don't generate enough heat to melt butter, he compliments Costner well. I never really understood Sela Ward as the neglected wife and felt she should of wanted Costner to quit out of concern for his safety as opposed to her selfish needs. But her presence on screen is a pleasure. The two unaccredited stars of this movie are the Coast Guard and the Sea. Both powerful forces which should not be taken for granted in real life or this movie. The movie has some slow spots and could have used the wasted 15 minutes to strengthen the character relationships. But it still works. The rescue scenes are intense and well filmed and edited to provide maximum impact. This movie earns the audience applause. And the applause of my two sons.\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xc4wQlkG2ezT"
      },
      "source": [
        "## Loading and Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrm5ibkx2fzS"
      },
      "source": [
        "The first step toward training any ML or DL model is to define a feature representation. This step has been relatively straightforward in the approaches we’ve seen so far, with BoW or embedding vectors. \n",
        "\n",
        "However, for neural networks, we need further processing of input vectors Let’s quickly recap the steps involved in converting training and test data into a format suitable for the neural network input layers:\n",
        "\n",
        "1. Tokenize the texts and convert them into word index vectors.\n",
        "2. Pad the text sequences so that all text vectors are of the same length.\n",
        "3. Map every word index to an embedding vector. We do that by multiplying word\n",
        "index vectors with the embedding matrix. The embedding matrix can either be\n",
        "populated using pre-trained embeddings or it can be trained for embeddings on\n",
        "this corpus.\n",
        "4. Use the output from Step 3 as the input to a neural network architecture.\n",
        "\n",
        "Once these are done, we can proceed with the specification of neural network architectures and training classifiers with them.\n",
        "\n",
        "Let's vectorize these text samples into a 2D integer tensor using Keras Tokenizer.\n",
        "\n",
        "Tokenizer is fit on training data only, and that is used to tokenize both train and test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CAeGu_Gs9A7F",
        "outputId": "9c2ee0a6-9220-43c7-a456-3b9e734d8e72",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Step-1: Tokenize the texts and convert them into word index vectors.\n",
        "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
        "tokenizer.fit_on_texts(train_texts)\n",
        "\n",
        "# Converting text to a vector of word indexes\n",
        "train_sequences = tokenizer.texts_to_sequences(train_texts)  \n",
        "test_sequences = tokenizer.texts_to_sequences(test_texts)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print(\"Found %s unique tokens.\" % len(word_index))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 88582 unique tokens.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4gilCBF-XEl"
      },
      "source": [
        "Now, we will convert this to sequences to be fed into neural network. Max sequence length is 1000 as set earlier initial padding of 0s, until vector is of size `MAX_SEQUENCE_LENGTH`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GiUk6lep9ztW",
        "outputId": "f5077098-59f5-469d-a6e4-998d9f1e38aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Step-2: Pad the text sequences so that all text vectors are of the same length.\n",
        "trainvalid_data = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "test_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "trainvalid_labels = to_categorical(np.asarray(train_labels))\n",
        "test_labels = to_categorical(np.asarray(test_labels))\n",
        "\n",
        "# split the training data into a training set and a validation set\n",
        "indices = np.arange(trainvalid_data.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "\n",
        "trainvalid_data = trainvalid_data[indices]\n",
        "trainvalid_labels = trainvalid_labels[indices]\n",
        "\n",
        "num_validation_samples = int(VALIDATION_SPLIT * trainvalid_data.shape[0])\n",
        "\n",
        "# This is the data we will use for CNN and RNN training\n",
        "x_train = trainvalid_data[:-num_validation_samples]\n",
        "y_train = trainvalid_labels[:-num_validation_samples]\n",
        "x_val = trainvalid_data[-num_validation_samples:]\n",
        "y_val = trainvalid_labels[-num_validation_samples:]\n",
        "\n",
        "print(\"Training set : \", (x_train.shape, y_train.shape))\n",
        "print(\"Validation set : \", (x_val.shape, y_val.shape))\n",
        "print(\"Splitting the train data into train and valid is done\")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training set :  ((20000, 1000), (20000, 2))\n",
            "Validation set :  ((5000, 1000), (5000, 2))\n",
            "Splitting the train data into train and valid is done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGfCrKb2EhSK"
      },
      "source": [
        "**Step 3**: If we want to use pre-trained embeddings to convert the train and test data into an embedding matrix like we did in the earlier examples with Word2vec and fastText, we have to download them and use them to convert our data into the input format for the neural networks.\n",
        "\n",
        "GloVe embeddings come with multiple dimensionalities, and we chose 100 as our dimension here. The value of dimensionality is a hyperparameter, and we can experiment with other dimensions as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MK9nQ3iXEMa8",
        "outputId": "11e64f5f-b939-46db-9409-96ae879e3982",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Step-3: Map every word index to an embedding vector. We do that by multiplying word index vectors with the embedding matrix. \n",
        "# The embedding matrix can either be populated using pre-trained embeddings or it can be trained for embeddings on this corpus.\n",
        "\n",
        "# first, build index mapping words in the embeddings set to their embedding vector\n",
        "embeddings_index = {}\n",
        "with open(os.path.join(GLOVE_DIR, \"glove.6B.100d.txt\")) as f:\n",
        "  for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype=\"float32\")\n",
        "    embeddings_index[word] = coefs\n",
        "print(\"Found %s word vectors in Glove embeddings.\" % len(embeddings_index))\n",
        "print(embeddings_index[\"google\"])\n",
        "\n",
        "# prepare embedding matrix - rows are the words from word_index, columns are the embeddings of that word from glove.\n",
        "num_words = min(MAX_NUM_WORDS, len(word_index)) + 1\n",
        "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
        "for word, i in word_index.items():\n",
        "  if i > MAX_NUM_WORDS:\n",
        "    continue\n",
        "  embedding_vector = embeddings_index.get(word)\n",
        "  if embedding_vector is not None:\n",
        "    # words not found in embedding index will be all-zeros.\n",
        "    embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors in Glove embeddings.\n",
            "[ 0.22575  -0.56253  -0.05156  -0.079389  1.1876   -0.48397  -0.23342\n",
            " -0.85278   0.97495  -0.33344   0.71692   0.12644   0.31962  -1.4136\n",
            " -0.57903  -0.037286 -0.0164    0.45155  -0.29005   0.52599  -0.22534\n",
            " -0.29556  -0.032407  1.5608   -0.013499 -0.064558  0.26625   0.78595\n",
            " -0.71693  -0.93025   0.80461   1.6035   -0.30602  -0.34764   0.93872\n",
            "  0.38137  -0.26743  -0.56519   0.58899  -0.14554  -0.34324   0.21291\n",
            " -0.39887   0.090042 -0.8495    0.38803  -0.5045   -0.22488   1.0644\n",
            " -0.2624    1.0334    0.06348  -0.39989   0.24236  -0.65636  -1.8107\n",
            " -0.061801  0.13795   1.1658   -0.30046  -0.50143   0.16509   0.039835\n",
            "  0.62541   0.56935   0.64125   0.21308   0.30276   0.39673   0.38973\n",
            "  0.28183   0.79481  -0.11962  -0.49598  -0.53195  -0.14897   0.51254\n",
            " -0.39208  -0.58535  -0.078509  0.81721  -0.73497  -0.68131   0.099243\n",
            " -0.87608   0.029632  0.33402  -0.14305   0.16964  -0.035178  0.39777\n",
            "  0.71769   0.25867  -0.36201   0.45698  -0.39156  -0.49343  -0.11224\n",
            "  0.29046   0.73216 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCocFE_nzwRs"
      },
      "source": [
        "The input layer for textual input is typically an embedding layer. The output layer, especially in the context of text classification, is a softmax layer with categorical output. If we want to train the input layer instead of using pre-trained embeddings, the easiest way is to call the Embedding layer class in Keras, specifying the input and output dimensions.\n",
        "\n",
        "However, since we want to use pre-trained embeddings, we should create a custom\n",
        "embedding layer that uses the embedding matrix we just built."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wghK5bfbyXQb",
        "outputId": "3c7712e0-62e9-4002-9cbd-08c6edfe0a2e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Step-4: load these pre-trained word embeddings into an Embedding layer, note that we set trainable = False so as to keep the embeddings fixed\n",
        "embedding_layer = Embedding(num_words, EMBEDDING_DIM, embeddings_initializer=Constant(embedding_matrix), input_length=MAX_SEQUENCE_LENGTH, trainable=False)\n",
        "print(\"Preparing of embedding matrix is done\")"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Preparing of embedding matrix is done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCSyidPq1xP6"
      },
      "source": [
        "This will serve as the input layer for any neural network we want to use (CNN or\n",
        "LSTM). Now that we know how to pre-process the input and define an input layer.\n",
        "\n",
        "let’s move on to specifying the rest of the neural network architecture using CNNs and LSTMs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLImbL5M11gh"
      },
      "source": [
        "## CNNs for Text Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DX2TTeQt13s_"
      },
      "source": [
        "CNNs typically consist of a series of convolution and pooling layers as the hidden layers. In the context of text classification, CNNs can be thought of as learning the most useful bag-of-words/n-grams features instead of taking the entire collection of words/n-grams as features. \n",
        "\n",
        "Since our dataset has only two classes—positive and negative—the output layer has two outputs, with the softmax activation function. We’ll define a CNN with three convolution-pooling layers using the Sequential model class in Keras, which allows us to specify DL models as a sequential stack of layers—one after another. \n",
        "\n",
        "Once the layers and their activation functions are specified, the next task is to define other important parameters, such as the optimizer, loss function, and the evaluation metric to tune the hyperparameters of the model. Once all this is done, the next step is to train and evaluate the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rqpIn3m1dkJ"
      },
      "source": [
        "cnnmodel = Sequential()\n",
        "cnnmodel.add(embedding_layer)\n",
        "\n",
        "cnnmodel.add(Conv1D(128, 5, activation=\"relu\"))\n",
        "cnnmodel.add(MaxPooling1D(5))\n",
        "\n",
        "cnnmodel.add(Conv1D(128, 5, activation=\"relu\"))\n",
        "cnnmodel.add(MaxPooling1D(5))\n",
        "\n",
        "cnnmodel.add(Conv1D(128, 5, activation=\"relu\"))\n",
        "cnnmodel.add(GlobalMaxPooling1D())\n",
        "\n",
        "cnnmodel.add(Dense(128, activation=\"relu\"))\n",
        "cnnmodel.add(Dense(len(labels_index), activation=\"softmax\"))\n",
        "\n",
        "cnnmodel.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\", metrics=[\"accuracy\"])\n",
        "\n",
        "print(f\"Train on {len(x_train)} samples, validate on {len(test_data)} samples\")\n",
        "# Train the model. Tune to validation set. \n",
        "cnnmodel.fit(x_train, y_train, batch_size=128, epochs=1, validation_data=(x_val, y_val))\n",
        "\n",
        "# Evaluate on test set:\n",
        "score, acc = cnnmodel.evaluate(test_data, test_labels)\n",
        "print(\"Test accuracy with CNN:\", acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RD0jt0aL9bQ2"
      },
      "source": [
        "A good approach while building your models is to experiment with different settings (i.e., hyperparameters). Keep in mind that all these decisions come with some associated cost. \n",
        "\n",
        "For example, in practice, we have the number of epochs as 10 or above. But that also increases the amount of time it takes to train the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_bQlCUP-auX"
      },
      "source": [
        "## CNN model with training your own embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMoRWVdc-by6"
      },
      "source": [
        "Another thing to note is that, if you want to train an embedding layer instead of using pretrained embeddings in this model, the only thing that changes is the line `cnnmodel.add(embedding_layer)`. Instead, we can specify a new embedding layer as, for example, `cnnmodel.add(Embedding(Param1, Param2))`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvG3vFs46M5U"
      },
      "source": [
        "print(\"Defining and training a CNN model, training embedding layer on the fly instead of using pre-trained embeddings\")\n",
        "cnnmodel = Sequential()\n",
        "cnnmodel.add(Embedding(MAX_NUM_WORDS, 128))\n",
        "\n",
        "cnnmodel.add(Conv1D(128, 5, activation=\"relu\"))\n",
        "cnnmodel.add(MaxPooling1D(5))\n",
        "\n",
        "cnnmodel.add(Conv1D(128, 5, activation=\"relu\"))\n",
        "cnnmodel.add(MaxPooling1D(5))\n",
        "\n",
        "cnnmodel.add(Conv1D(128, 5, activation=\"relu\"))\n",
        "cnnmodel.add(GlobalMaxPooling1D())\n",
        "\n",
        "cnnmodel.add(Dense(128, activation=\"relu\"))\n",
        "cnnmodel.add(Dense(len(labels_index), activation=\"softmax\"))\n",
        "\n",
        "cnnmodel.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Train the model. Tune to validation set. \n",
        "cnnmodel.fit(x_train, y_train, batch_size=128, epochs=1, validation_data=(x_val, y_val))\n",
        "\n",
        "# Evaluate on test set:\n",
        "score, acc = cnnmodel.evaluate(test_data, test_labels)\n",
        "print(\"Test accuracy with CNN:\", acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HiPbx0XCAmh_"
      },
      "source": [
        "We’ll notice that, in this case, training the embedding layer on our own dataset seems to result in better classification on test data.\n",
        "\n",
        "However, if the training data were substantially small, sticking to the pre-trained embeddings, or using the domain adaptation techniques, would be a better choice."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARzhslZ0Aytg"
      },
      "source": [
        "## LSTMs for Text Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQFUIRZNAz8n"
      },
      "source": [
        "LSTMs and other variants of RNNs in general have become the go-to way of doing neural language modeling in the past few years. This is primarily because language is sequential in nature and RNNs are specialized in\n",
        "working with sequential data. The current word in the sentence depends on its context—the words before and after. \n",
        "\n",
        "However, when we model text using CNNs, this crucial fact is not taken into account. RNNs work on the principle of using this context while learning the language representation or a model of language. Hence, they’re known to work well for NLP tasks.\n",
        "\n",
        "In this section, we’ll see an example of using RNNs for text classification. Now that we’ve already seen one neural network in action, it’s relatively easy to train another! Just replace the convolutional and pooling parts with an LSTM in the prior two code examples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_R1bQaUAQca"
      },
      "source": [
        "print(\"Defining and training an LSTM model, training embedding layer on the fly\")\n",
        "\n",
        "rnnmodel = Sequential()\n",
        "rnnmodel.add(Embedding(MAX_NUM_WORDS, 128))\n",
        "\n",
        "rnnmodel.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "rnnmodel.add(Dense(len(labels_index), activation=\"sigmoid\"))\n",
        "\n",
        "rnnmodel.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Train the model. Tune to validation set. \n",
        "rnnmodel.fit(x_train, y_train, batch_size=32, epochs=1, validation_data=(x_val, y_val))\n",
        "\n",
        "# Evaluate on test set:\n",
        "score, acc = rnnmodel.evaluate(test_data, test_labels, batch_size=32)\n",
        "print(\"Test accuracy with RNN:\", acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzwEfo97EUzM"
      },
      "source": [
        "## LSTM Model using pre-trained Embedding Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCZvOX8sE6rH"
      },
      "source": [
        "print(\"Defining and training an LSTM model, using pre-trained embedding layer\")\n",
        "\n",
        "rnnmodel2 = Sequential()\n",
        "rnnmodel2.add(embedding_layer)\n",
        "\n",
        "rnnmodel2.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "rnnmodel2.add(Dense(len(labels_index), activation=\"sigmoid\"))\n",
        "\n",
        "rnnmodel2.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Train the model. Tune to validation set. \n",
        "rnnmodel2.fit(x_train, y_train, batch_size=32, epochs=1, validation_data=(x_val, y_val))\n",
        "\n",
        "# Evaluate on test set:\n",
        "score, acc = rnnmodel2.evaluate(test_data, test_labels, batch_size=32)\n",
        "print(\"Test accuracy with RNN:\", acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7tnGcfOE61j"
      },
      "source": [
        "While LSTMs are more powerful in utilizing the sequential nature of text, they’re much more data hungry as compared to CNNs. Thus, the relative lower performance of the LSTM on a dataset need not necessarily be interpreted as a shortcoming of the model itself. It’s possible that the amount of data we have is not sufficient to utilize the full potential of an LSTM."
      ]
    }
  ]
}