{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4-document-embeddings.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMoBtRne4Y9PRwVz8Trsq61",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/practical-natural-language-processing/blob/chapter-4-text-classification/4_document_embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiYivTWAxkWN"
      },
      "source": [
        "# Using Neural Embeddings in Text Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdDel-Z4xlPn"
      },
      "source": [
        "As we already know that feature engineering techniques based on using neural networks, such as word embeddings, character embeddings, and document embeddings. The advantage of using embedding-based features is that they create a dense, low-dimensional feature representation instead of the sparse, highdimensional structure of BoW/TF-IDF and other such features. There are different ways of designing and using features based on neural embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsy2tf7j01jV"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVm9nSvc03mC",
        "outputId": "96c12b62-bc56-4d70-b890-3aab98a4f8c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHA0UTRq0AuJ"
      },
      "source": [
        "## Document Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSOkRJRU0EK8"
      },
      "source": [
        "In the Doc2vec embedding scheme, we learn a direct representation for the entire document (sentence/paragraph) rather than each word. Just as we used word and character embeddings as features for performing text classification, we can also use Doc2vec as a feature representation mechanism. \n",
        "\n",
        "Since there are no existing pretrained models that work with the latest version of Doc2vec, let’s see how to build our own Doc2vec model and use it for text classification.\n",
        "\n",
        "We’ll use a dataset called “Sentiment Analysis: Emotion in Text” from [figureeight.com](https://appen.com/), which contains 40,000 tweets labeled with 13 labels signifying different emotions. Let’s take the three most frequent labels in this dataset—neutral, worry, happiness—and build a text classifier for classifying new tweets into one of these three classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GB383BAN1hxQ",
        "outputId": "0c1e2982-1e88-4283-f871-2d944cdf86f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 517
        }
      },
      "source": [
        "# downloding data\n",
        "!wget -P DATAPATH https://raw.githubusercontent.com/practical-nlp/practical-nlp/master/Ch4/Data/Sentiment%20and%20Emotion%20in%20Text/train_data.csv\n",
        "!wget -P DATAPATH https://raw.githubusercontent.com/practical-nlp/practical-nlp/master/Ch4/Data/Sentiment%20and%20Emotion%20in%20Text/test_data.csv\n",
        "!ls -lah DATAPATH"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-10-09 10:01:09--  https://raw.githubusercontent.com/practical-nlp/practical-nlp/master/Ch4/Data/Sentiment%20and%20Emotion%20in%20Text/train_data.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2479133 (2.4M) [text/plain]\n",
            "Saving to: ‘DATAPATH/train_data.csv’\n",
            "\n",
            "train_data.csv      100%[===================>]   2.36M  6.00MB/s    in 0.4s    \n",
            "\n",
            "2020-10-09 10:01:10 (6.00 MB/s) - ‘DATAPATH/train_data.csv’ saved [2479133/2479133]\n",
            "\n",
            "--2020-10-09 10:01:10--  https://raw.githubusercontent.com/practical-nlp/practical-nlp/master/Ch4/Data/Sentiment%20and%20Emotion%20in%20Text/test_data.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 783640 (765K) [text/plain]\n",
            "Saving to: ‘DATAPATH/test_data.csv’\n",
            "\n",
            "test_data.csv       100%[===================>] 765.27K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2020-10-09 10:01:11 (5.09 MB/s) - ‘DATAPATH/test_data.csv’ saved [783640/783640]\n",
            "\n",
            "total 3.2M\n",
            "drwxr-xr-x 2 root root 4.0K Oct  9 10:01 .\n",
            "drwxr-xr-x 1 root root 4.0K Oct  9 10:01 ..\n",
            "-rw-r--r-- 1 root root 766K Oct  9 10:01 test_data.csv\n",
            "-rw-r--r-- 1 root root 2.4M Oct  9 10:01 train_data.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYiRNwTR11xZ",
        "outputId": "0d23fcfa-4a01-45e0-d5f9-b09a616ef68a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "source": [
        "#Load the dataset and explore.\n",
        "filepath = \"DATAPATH/train_data.csv\"\n",
        "df = pd.read_csv(filepath)\n",
        "print(df.shape)\n",
        "df.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(30000, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "      <th>content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>empty</td>\n",
              "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>sadness</td>\n",
              "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>sadness</td>\n",
              "      <td>Funeral ceremony...gloomy friday...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>enthusiasm</td>\n",
              "      <td>wants to hang out with friends SOON!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>neutral</td>\n",
              "      <td>@dannycastillo We want to trade with someone w...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    sentiment                                            content\n",
              "0       empty  @tiffanylue i know  i was listenin to bad habi...\n",
              "1     sadness  Layin n bed with a headache  ughhhh...waitin o...\n",
              "2     sadness                Funeral ceremony...gloomy friday...\n",
              "3  enthusiasm               wants to hang out with friends SOON!\n",
              "4     neutral  @dannycastillo We want to trade with someone w..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHTrPtVvzt2O",
        "outputId": "3fb62ef2-fe10-4e94-e9bc-53ebf086c3a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        }
      },
      "source": [
        "df['sentiment'].value_counts()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "worry         7433\n",
              "neutral       6340\n",
              "sadness       4828\n",
              "happiness     2986\n",
              "love          2068\n",
              "surprise      1613\n",
              "hate          1187\n",
              "fun           1088\n",
              "relief        1021\n",
              "empty          659\n",
              "enthusiasm     522\n",
              "boredom        157\n",
              "anger           98\n",
              "Name: sentiment, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqgCo2oEzyku",
        "outputId": "1aecaeae-a5be-4cd9-e80a-d95d5f5ab823",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Let us take the top 3 categories and leave out the rest.\n",
        "shortlist = [\"neutral\", \"happiness\", \"worry\"]\n",
        "df_subset = df[df[\"sentiment\"].isin(shortlist)]\n",
        "df_subset.shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(16759, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9SodXsJ0bXq"
      },
      "source": [
        "## Text pre-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oq4fP9xG0cQ_"
      },
      "source": [
        "Our traditional tokenizers may not work well with tweets, splitting smileys, hashtags, Twitter handles, etc., into multiple tokens. Such specialized needs prompted a lot of research into NLP for Twitter in the\n",
        "recent past, which resulted in several pre-processing options for tweets. One such solution is a TweetTokenizer, implemented in the NLTK library in Python.\n",
        "\n",
        "Tweets are different. Somethings to consider:\n",
        "\n",
        "- Removing @mentions, and urls perhaps?\n",
        "- using NLTK Tweet tokenizer instead of a regular one\n",
        "- stopwords, numbers as usual."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjCUKBJq0KLi"
      },
      "source": [
        "# strip_handles removes personal information such as twitter handles, which don't\n",
        "# contribute to emotion in the tweet. preserve_case=False converts everything to lowercase.\n",
        "tweeter = TweetTokenizer(strip_handles=True, preserve_case=False)\n",
        "mystopwords = set(stopwords.words(\"english\"))\n",
        "\n",
        "# Function to tokenize tweets, remove stopwords and numbers. \n",
        "# Keeping punctuations and emoticon symbols could be relevant for this task!\n",
        "def preprocess_corpus(texts):\n",
        "\n",
        "  def remove_stops_digits(tokens):\n",
        "    # Nested function that removes stopwords and digits from a list of tokens\n",
        "    return [token for token in tokens if token not in mystopwords and not token.isdigit()]\n",
        "\n",
        "  # This return statement below uses the above function to process twitter tokenizer output further.\n",
        "  return [remove_stops_digits(tweeter.tokenize(content)) for content in texts]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvo7CBCN2MKO",
        "outputId": "ee8d5cfb-0170-4033-99ea-16955adae18a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# df_subset contains only the three categories we chose.\n",
        "mydata = preprocess_corpus(df_subset[\"content\"])\n",
        "mycats = df_subset[\"sentiment\"]\n",
        "print(len(mydata), len(mycats))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16759 16759\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mj8RCuG-3c4k"
      },
      "source": [
        "## Training Doc2vec classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFT2qNYD3i0u"
      },
      "source": [
        "The next step in this process is to train a Doc2vec model to learn tweet representations. Ideally, any large dataset of tweets will work for this step. However, since we don’t have such a ready-made corpus, we’ll split our dataset into train-test and use the training data for learning the Doc2vec representations. \n",
        "\n",
        "The first part of this process involves converting the data into a format readable by the Doc2vec implementation, which can be done using the TaggedDocument class. It’s used to represent a document as a list of tokens, followed by a “tag,” which in its simplest form can be just the filename or ID of the document. \n",
        "\n",
        "However, Doc2vec by itself can also be used as a nearest neighbor classifier for both multiclass and multilabel classification problems using . We’ll leave this as an exploratory exercise for the reader. \n",
        "\n",
        "Let’s now see how to train a Doc2vec classifier for tweets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cyf6aMuv2f_t",
        "outputId": "a01188c3-ce35-4498-b86d-d1738461a470",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "# Split data into train and test, following the usual process\n",
        "train_data, test_data, train_cats, test_cats = train_test_split(mydata, mycats, random_state=1234)\n",
        "\n",
        "# prepare training data in doc2vec format\n",
        "train_doc2vec = [TaggedDocument((d), tags=[str(i)]) for i, d in enumerate(train_data)]\n",
        "\n",
        "# Train a doc2vec model to learn tweet representations. Use only training data!!\n",
        "model = Doc2Vec(vector_size=50, alpha=0.025, min_count=5, dm=1, epochs=100)\n",
        "model.build_vocab(train_doc2vec)\n",
        "model.train(train_doc2vec, total_examples=model.corpus_count, epochs=model.epochs)\n",
        "model.save(\"d2v.model\")\n",
        "print(\"Model Saved\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model Saved\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:252: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhCI37c25kx4"
      },
      "source": [
        "Doc2vec’s infer_vector function can be used to infer the vector representation for a given text using a pre-trained model. Since there is some amount of randomness due to the choice of hyperparameters, the inferred vectors differ each time we extract them. For this reason, to get a stable representation, we run it multiple times (called steps) and aggregate the vectors.\n",
        "\n",
        "Let’s use the learned model to infer features for our data and train a logistic regression classifier:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJQNqC3-5CmN",
        "outputId": "0b6b8bab-6dcd-421c-a54c-aec9cd1c7914",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        }
      },
      "source": [
        "# Infer the feature representation for training and test data using the trained model\n",
        "model = Doc2Vec.load(\"d2v.model\")\n",
        "\n",
        "# infer in multiple steps to get a stable representation.\n",
        "train_vectors = [model.infer_vector(list_of_tokens, steps=50) for list_of_tokens in train_data]\n",
        "test_vectors = [model.infer_vector(list_of_tokens, steps=50) for list_of_tokens in test_data]\n",
        "\n",
        "# Use any regular classifier like logistic regression\n",
        "myclass = LogisticRegression(class_weight=\"balanced\")  # because classes are not balanced. \n",
        "myclass.fit(train_vectors, train_cats)\n",
        "\n",
        "preds = myclass.predict(test_vectors)\n",
        "print(classification_report(test_cats, preds))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:252: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "   happiness       0.33      0.53      0.40       713\n",
            "     neutral       0.46      0.53      0.49      1595\n",
            "       worry       0.60      0.39      0.47      1882\n",
            "\n",
            "    accuracy                           0.47      4190\n",
            "   macro avg       0.46      0.48      0.46      4190\n",
            "weighted avg       0.50      0.47      0.47      4190\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GD2NB2S7kwC"
      },
      "source": [
        "Now, the performance of this model seems rather poor, achieving an F1 score of 0.51 on a reasonably large corpus, with only three classes. There are a couple of interpretations for this poor result. \n",
        "\n",
        "First, unlike full news articles or even well-formed sentences, tweets contain very little data per instance. \n",
        "\n",
        "Further, people write with a wide variety in spelling and syntax when they tweet. There are a lot of emoticons in different forms. Our feature representation should be able to capture such aspects. While tuning the algorithms by searching a large parameter space for the best model may\n",
        "help, an alternative could be to explore problem-specific feature representations.\n",
        "\n",
        "An important point to keep in mind when using Doc2vec is the same as for fastText: if we have to use Doc2vec for feature representation, we have to store the model that learned the representation. While it’s not typically as bulky as fastText, it’s also not as fast to train. Such trade-offs need to be considered and compared before we make a deployment decision."
      ]
    }
  ]
}