{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3-subword-embeddings-and-fast-text.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOUG1XNFKbMJ4S3FOFc5+zX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/practical-natural-language-processing/blob/chapter-4-text-classification/3_subword_embeddings_and_fast_text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uf5RHsLxlRBw"
      },
      "source": [
        "# Subword Embeddings and fastText"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3VjFEMNlsOl"
      },
      "source": [
        "**Word embeddings are about word representations**. Even offthe-\n",
        "shelf embeddings seem to work well on classification tasks. \n",
        "\n",
        "However, if a word in our dataset was not present in the pre-trained model’s vocabulary, how will we get a representation for this word? \n",
        "\n",
        "This problem is popularly known as out of vocabulary (OOV). In our previous example, we just ignored such words from feature extraction. Is there a better way?\n",
        "\n",
        "fastText embeddings is based on the idea of enriching word embeddings with subword-level information. Thus, the embedding representation for each word is represented as a sum of the representations of individual\n",
        "character n-grams. While this may seem like a longer process compared to just estimating word-level embeddings, it has two advantages:\n",
        "\n",
        "- This approach can handle words that did not appear in training data (OOV).\n",
        "- The implementation facilitates extremely fast learning on even very large\n",
        "corpora.\n",
        "\n",
        "While fastText is a general-purpose library to learn the embeddings, it also supports off-the-shelf text classification by providing end-to-end classifier training and testing; i.e., we don’t have to handle feature extraction separately."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFiny4g-nYxn"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxH_YHiRnbQh"
      },
      "source": [
        "In this notebook we will demonstrate using the fastText library to perform text classificatoin on the dbpedie data which can we downloaded from [here](https://github.com/le-scientifique/torchDatasets/raw/master/dbpedia_csv.tar.gz).\n",
        "fastText is a library for learning of word embeddings and text classification created by Facebook's AI Research (FAIR) lab. The model allows to create an unsupervised learning or supervised learning algorithm for obtaining vector representations for words. Facebook makes available pretrained models for 294 languages(source: [wiki](https://en.wikipedia.org/wiki/FastText)).\n",
        "\n",
        "Note: This notebook uses an older version of fasttext."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vD308E3Nnn05"
      },
      "source": [
        "!pip install fasttext==0.9.2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUBGINLPnt-0"
      },
      "source": [
        "import pandas as pd\n",
        "from fasttext import train_supervised"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEFDzm8un8vR",
        "outputId": "ad3e7d6e-65b5-4d14-daf7-ebc83b7ad4d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        }
      },
      "source": [
        "# downloading the data\n",
        "!wget -P DATAPATH https://github.com/le-scientifique/torchDatasets/raw/master/dbpedia_csv.tar.gz\n",
        "\n",
        "# untaring the reuqired file\n",
        "!tar -xvf DATAPATH/dbpedia_csv.tar.gz -C DATAPATH\n",
        "\n",
        "# sneek peek in the folder structure\n",
        "!ls -lah DATAPATH"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-10-08 09:52:56--  https://github.com/le-scientifique/torchDatasets/raw/master/dbpedia_csv.tar.gz\n",
            "Resolving github.com (github.com)... 52.69.186.44\n",
            "Connecting to github.com (github.com)|52.69.186.44|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://github.com/srhrshr/torchDatasets/raw/master/dbpedia_csv.tar.gz [following]\n",
            "--2020-10-08 09:52:56--  https://github.com/srhrshr/torchDatasets/raw/master/dbpedia_csv.tar.gz\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/srhrshr/torchDatasets/master/dbpedia_csv.tar.gz [following]\n",
            "--2020-10-08 09:52:56--  https://raw.githubusercontent.com/srhrshr/torchDatasets/master/dbpedia_csv.tar.gz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 68431223 (65M) [application/octet-stream]\n",
            "Saving to: ‘DATAPATH/dbpedia_csv.tar.gz’\n",
            "\n",
            "dbpedia_csv.tar.gz  100%[===================>]  65.26M  68.0MB/s    in 1.0s    \n",
            "\n",
            "2020-10-08 09:53:02 (68.0 MB/s) - ‘DATAPATH/dbpedia_csv.tar.gz’ saved [68431223/68431223]\n",
            "\n",
            "dbpedia_csv/\n",
            "dbpedia_csv/test.csv\n",
            "dbpedia_csv/classes.txt\n",
            "dbpedia_csv/train.csv\n",
            "dbpedia_csv/readme.txt\n",
            "total 66M\n",
            "drwxr-xr-x 3 root root 4.0K Oct  8 09:53 .\n",
            "drwxr-xr-x 1 root root 4.0K Oct  8 09:53 ..\n",
            "drwxrwxr-x 2 1000 1000 4.0K Mar 29  2015 dbpedia_csv\n",
            "-rw-r--r-- 1 root root  66M Oct  8 09:53 dbpedia_csv.tar.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-E2YKuKocVA"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-RxFrd2oLut",
        "outputId": "10f1316f-2a3f-4517-c3eb-29b40a38185f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "data_path = \"DATAPATH\"\n",
        "\n",
        "# Loading train data\n",
        "train_file = data_path + \"/dbpedia_csv/train.csv\"\n",
        "df = pd.read_csv(train_file, header=None, names=[\"class\", \"name\", \"description\"])\n",
        "\n",
        "# Loading test data\n",
        "test_file = data_path + \"/dbpedia_csv/test.csv\"\n",
        "df_test = pd.read_csv(test_file, header=None, names=[\"class\", \"name\", \"description\"])\n",
        "\n",
        "# Data we have\n",
        "print(\"Train:{} Test:{}\".format(df.shape, df_test.shape))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train:(560000, 3) Test:(70000, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8i9cgEGjpaUN",
        "outputId": "e4cac8a6-d095-4cd5-8ec2-edf21f63d842",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        }
      },
      "source": [
        "# Since we have no clue about the classes lets build one\n",
        "# Mapping from class number to class name\n",
        "class_dict = {\n",
        "    1:'Company',\n",
        "    2:'EducationalInstitution',\n",
        "    3:'Artist',\n",
        "    4:'Athlete',\n",
        "    5:'OfficeHolder',\n",
        "    6:'MeanOfTransportation',\n",
        "    7:'Building',\n",
        "    8:'NaturalPlace',\n",
        "    9:'Village',\n",
        "    10:'Animal',\n",
        "    11:'Plant',\n",
        "    12:'Album',\n",
        "    13:'Film',\n",
        "    14:'WrittenWork'\n",
        "}\n",
        "\n",
        "# Mapping the classes\n",
        "df[\"class_name\"] = df[\"class\"].map(class_dict)\n",
        "df.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>class</th>\n",
              "      <th>name</th>\n",
              "      <th>description</th>\n",
              "      <th>class_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>E. D. Abbott Ltd</td>\n",
              "      <td>Abbott of Farnham E D Abbott Limited was a Br...</td>\n",
              "      <td>Company</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Schwan-Stabilo</td>\n",
              "      <td>Schwan-STABILO is a German maker of pens for ...</td>\n",
              "      <td>Company</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>Q-workshop</td>\n",
              "      <td>Q-workshop is a Polish company located in Poz...</td>\n",
              "      <td>Company</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>Marvell Software Solutions Israel</td>\n",
              "      <td>Marvell Software Solutions Israel known as RA...</td>\n",
              "      <td>Company</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>Bergan Mercy Medical Center</td>\n",
              "      <td>Bergan Mercy Medical Center is a hospital loc...</td>\n",
              "      <td>Company</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   class  ... class_name\n",
              "0      1  ...    Company\n",
              "1      1  ...    Company\n",
              "2      1  ...    Company\n",
              "3      1  ...    Company\n",
              "4      1  ...    Company\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRjHaWnip3F0",
        "outputId": "001246c6-65ee-48fe-ec79-41924eed79c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        }
      },
      "source": [
        "df[\"class_name\"].value_counts()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Artist                    40000\n",
              "Company                   40000\n",
              "Athlete                   40000\n",
              "WrittenWork               40000\n",
              "Film                      40000\n",
              "Album                     40000\n",
              "Plant                     40000\n",
              "NaturalPlace              40000\n",
              "OfficeHolder              40000\n",
              "Animal                    40000\n",
              "MeanOfTransportation      40000\n",
              "Village                   40000\n",
              "EducationalInstitution    40000\n",
              "Building                  40000\n",
              "Name: class_name, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmXu6FLtqhYy"
      },
      "source": [
        "# Lets do some cleaning of this text\n",
        "def clean_it(text, normalize=True):\n",
        "  # Replacing possible issues with data. We can add or reduce the replacemtent in this chain\n",
        "  s = str(text).replace(',',' ').replace('\"','').replace('\\'',' \\' ').replace('.',' . ').replace('(',' ( ').\\\n",
        "            replace(')',' ) ').replace('!',' ! ').replace('?',' ? ').replace(':',' ').replace(';',' ').lower()\n",
        "\n",
        "  # normalizing / encoding the text\n",
        "  if normalize:\n",
        "    s = s.normalize(\"NFKD\").str.encode(\"ascii\", \"ignore\").str.decode(\"utf-8\")\n",
        "  \n",
        "  return s"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkr3dwR4sgv0"
      },
      "source": [
        "# Now lets define a small function where we can use above cleaning on datasets\n",
        "def clean_df(data, cleanit=False, shuffleit=False, encodeit=False, label_prefix=\"__class__\"):\n",
        "  # Defining the new data\n",
        "  df = data[[\"name\", \"description\"]].copy(deep=True)\n",
        "  df[\"class\"] = label_prefix + data[\"class\"].astype(str) + \" \"\n",
        "\n",
        "  # cleaning it\n",
        "  if cleanit:\n",
        "    df[\"name\"] = df[\"name\"].apply(lambda x: clean_it(x, encodeit))\n",
        "    df[\"description\"] = df[\"description\"].apply(lambda x: clean_it(x, encodeit))\n",
        "\n",
        "  # shuffling it\n",
        "  if shuffleit:\n",
        "    df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "  return df"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbkjjTONu6f1",
        "outputId": "ed0ce100-c8ad-4ba9-dc39-ea61dc90b243",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "# Transform the datasets using the above clean functions\n",
        "df_train_cleaned = clean_df(df, True, True)\n",
        "df_test_cleaned = clean_df(df_test, True, True)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 6.73 s, sys: 1.06 s, total: 7.79 s\n",
            "Wall time: 7.97 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RlE2y5HFvMmc"
      },
      "source": [
        "# Write files to disk as fastText classifier API reads files from disk.\n",
        "train_file = data_path + \"/dbpedia_train.csv\"\n",
        "df_train_cleaned.to_csv(train_file, header=None, index=False, columns=[\"class\", \"name\", \"description\"])\n",
        "\n",
        "test_file = data_path + \"/dbpedia_test.csv\"\n",
        "df_test_cleaned.to_csv(test_file, header=None, index=False, columns=[\"class\", \"name\", \"description\"])"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Oee7FkmwuYa"
      },
      "source": [
        "## Feature extraction and training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lP1CeMGwwkC"
      },
      "source": [
        "Now that we have the train and test files written into disk in a format fastText wants, we are ready to use it for text classification!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0AgfMkWBwE4w",
        "outputId": "9513c3e9-02a9-46fb-f342-684cde9029da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "# Using fastText for feature extraction and training\n",
        "model = train_supervised(input=train_file, label=\"__class__\", lr=1.0, epoch=75, loss=\"ova\", wordNgrams=2, dim=200, thread=2, verbose=100)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1h 11min 2s, sys: 17.6 s, total: 1h 11min 20s\n",
            "Wall time: 36min 32s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCxwb_OkzBem",
        "outputId": "456e8513-de4c-499f-a39e-8d44b503f999",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "for k in range(1, 6):\n",
        "  results = model.test(test_file, k=k)\n",
        "  print(f\"Test Samples: {results[0]} Precision@{k} : {results[1]*100:2.4f} Recall@{k} : {results[2]*100:2.4f}\")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Samples: 70000 Precision@1 : 93.0371 Recall@1 : 93.0371\n",
            "Test Samples: 70000 Precision@2 : 48.5507 Recall@2 : 97.1014\n",
            "Test Samples: 70000 Precision@3 : 32.5076 Recall@3 : 97.5229\n",
            "Test Samples: 70000 Precision@4 : 24.4550 Recall@4 : 97.8200\n",
            "Test Samples: 70000 Precision@5 : 19.6106 Recall@5 : 98.0529\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvULInp2zNdr"
      },
      "source": [
        "We’ll notice that, despite the fact that this is a huge dataset and we gave the classifier raw text and not the feature vector, the training\n",
        "takes only a few seconds, and we get close to 98% precision and recall!\n",
        "\n",
        "When we have a large dataset, and when learning seems infeasible with the\n",
        "approaches described so far, fastText is a good option to use to set up a strong working baseline.\n",
        "\n",
        "However, there’s one concern to keep in mind when using fastText, as\n",
        "was the case with Word2vec embeddings: it uses pre-trained character n-gram\n",
        "embeddings. Thus, when we save the trained model, it carries the entire character ngram embeddings dictionary with it. This results in a bulky model and can result in engineering issues.\n",
        "\n",
        "> **fastText is extremely fast to train and very useful for setting up\n",
        "strong baselines. The downside is the model size.**\n",
        "\n",
        "Now try training a classifier on this dataset with, say, LogisticRegression to realize how fast fastText is! 93% Precision and Recall are hard numbers to beat, too!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Fh1mO_J0fcz"
      },
      "source": [
        "# TODO"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}